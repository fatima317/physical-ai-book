# Glossary of Terms

This glossary provides definitions for key terms used throughout the Physical AI & Humanoid Robotics curriculum.

## A

**Action**: In ROS 2, a goal-oriented communication pattern that allows for long-running tasks with feedback and status updates.

**Artificial Intelligence (AI)**: The simulation of human intelligence processes by machines, especially computer systems, particularly useful in robotics for decision making and learning.

**Autonomous System**: A system that operates independently without human intervention, typically using sensors and AI to navigate and perform tasks.

## B

**Behavior Tree**: A hierarchical tree-like structure used to organize and control the execution of tasks in robotics and AI systems.

**Buffer**: A region of memory used to temporarily hold data while it's being transferred from one place to another, commonly used in robotics for storing sensor data.

## C

**Computer Vision**: A field of AI that trains computers to interpret and understand the visual world, using digital images and deep learning models.

**Control System**: A system that manages, commands, directs, or regulates the behavior of other devices or systems to achieve desired outcomes.

**Convolutional Neural Network (CNN)**: A class of deep neural networks commonly applied to visual imagery in robotics applications.

## D

**Deep Learning**: A subset of machine learning based on artificial neural networks with representation learning, used extensively in robotics perception.

**Depth Perception**: The ability to perceive the world in three dimensions and judge distance, crucial for robot navigation and manipulation.

**Differential Drive**: A common wheel configuration for mobile robots where each side of the robot has an independent drive system.

## E

**Embodied AI**: Artificial intelligence that is integrated with a physical body and interacts with the real world through sensors and actuators.

**End Effector**: The device at the end of a robotic arm designed to interact with the environment, such as a gripper or tool.

**Encoder**: A device that measures the position, velocity, or acceleration of a rotating shaft or linear movement in robotics.

## F

**Finite State Machine (FSM)**: A computational model used to design algorithms and control robot behavior through discrete states and transitions.

**Forward Kinematics**: The use of kinematic equations to compute the position of the end-effector from specified values of joint parameters.

**Fiducial Marker**: A visual marker with a known pattern used for localization and pose estimation in robotics.

## G

**Gazebo**: A 3D simulation environment for robotics that provides realistic physics, high-quality graphics, and convenient programmatic interfaces.

**Global Planner**: An algorithm that computes a path from a start to goal position considering the entire map of the environment.

## H

**Human-Robot Interaction (HRI)**: The study of interactions between humans and robots, focusing on design, development, and evaluation of robotic systems for human use.

**Hardware Abstraction Layer (HAL)**: Software layer that allows higher-level software to interact with hardware without knowing the specifics of the hardware.

## I

**Inverse Kinematics**: The mathematical process of calculating the variable joint parameters needed to place the end of a kinematic chain in a given position.

**Inertial Measurement Unit (IMU)**: An electronic device that measures and reports a body's specific force, angular rate, and sometimes the magnetic field surrounding the body.

**Intelligent Agent**: An autonomous entity that perceives its environment and takes actions that maximize its chance of achieving its goals.

## J

**Joint State**: Information about the position, velocity, and effort of each joint in a robotic system.

**Jacobian Matrix**: A matrix of all first-order partial derivatives of a vector-valued function, used in robotics for kinematic transformations.

## K

**Kinematics**: The study of motion without considering the forces that cause it, fundamental to robot arm and leg control.

**Kalman Filter**: An algorithm that uses a series of measurements observed over time to estimate unknown variables, widely used in robotics for sensor fusion.

## L

**LiDAR**: Light Detection and Ranging, a remote sensing method that uses light in the form of a pulsed laser to measure distances.

**Localization**: The process of determining the position and orientation of a robot in a known map.

**LQR (Linear Quadratic Regulator)**: An optimal control technique used to control linear systems with quadratic cost functions.

## M

**Machine Learning**: A method of training algorithms to learn from and make predictions or decisions based on data, essential in robotics.

**Manipulation**: The branch of robotics concerned with the way robots handle objects in their environment.

**Mapping**: The process of creating a representation of the environment for navigation and planning purposes.

**MPC (Model Predictive Control)**: An advanced method of process control that uses a model of the system to predict future behavior.

## N

**Navigation**: The ability of a robot to move from one location to another while avoiding obstacles.

**Neural Network**: A series of algorithms that endeavors to recognize underlying relationships in a set of data through a process that mimics how the human brain operates.

**Node**: In ROS, a process that performs computation, nodes are the fundamental building blocks of ROS programs.

## O

**Odometry**: The use of data from motion sensors to estimate change in position over time, commonly used for robot localization.

**Occupancy Grid**: A probabilistic 2D or 3D representation of space used for navigation and obstacle avoidance.

**OpenCV**: Open Source Computer Vision Library, widely used in robotics for image processing and computer vision tasks.

## P

**Path Planning**: The computational problem of finding a feasible and optimal path from a start to a goal location.

**PID Controller**: A control loop mechanism employing feedback that is widely used in robotics and industrial control systems.

**Point Cloud**: A set of data points in space, commonly produced by 3D scanners and LiDAR sensors in robotics.

**Perception Pipeline**: A sequence of processing steps that transform raw sensor data into meaningful information about the environment.

## Q

**Quaternion**: A mathematical concept used to represent rotations and orientations in 3D space, avoiding gimbal lock issues.

## R

**Robot Operating System (ROS)**: Flexible framework for writing robot software, providing services designed for a heterogeneous computer cluster.

**ROS 2**: The second generation of ROS, designed for production environments with improved security and real-time capabilities.

**Reinforcement Learning**: A type of machine learning where an agent learns to make decisions by performing actions and receiving rewards.

**RRT (Rapidly-exploring Random Tree)**: A motion planning algorithm used for path planning in robotics.

**RGB-D**: A combination of traditional RGB color information with depth information for each pixel.

## S

**Sensor Fusion**: The process of combining sensory data or data derived from disparate sources to achieve better information than could be achieved by means of one source alone.

**SLAM (Simultaneous Localization and Mapping)**: The computational problem of constructing or updating a map of an unknown environment while simultaneously keeping track of an agent's location.

**State Estimation**: The process of estimating the internal state of a dynamical system from noisy observations.

**Service**: In ROS, a synchronous request/reply communication pattern between nodes.

## T

**Trajectory**: A time-parameterized path that describes how a robot should move through space over time.

**Teleoperation**: The remote operation of a robot by a human operator.

**Twist Message**: In ROS, a message type that represents the velocity of a rigid body, including both linear and angular components.

## V

**Vision-Language-Action (VLA)**: Systems that integrate visual perception, language understanding, and physical action in robotics.

**Velocity Controller**: A controller that manages the velocity of robot joints or base movement.

**VSLAM (Visual SLAM)**: Simultaneous Localization and Mapping using visual sensors as the primary input.

## W

**Waypoint**: A specified geographical location used for route definition in navigation.

**Wheel Odometry**: Odometry calculated based on the rotation of wheels, commonly used for mobile robot localization.

## Y

**Yaw**: The rotation around the vertical axis of a robot or vehicle, part of the 3D orientation (pitch, roll, yaw).

## Z

**Zero Moment Point (ZMP)**: A criterion for static and dynamic stability of legged robots, important in humanoid robotics.