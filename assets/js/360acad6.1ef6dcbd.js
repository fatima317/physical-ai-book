"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[6143],{7149:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"weekly-breakdown/week5/index","title":"Week 5 - Computer Vision for Robotics","description":"Learning Objectives","source":"@site/docs/weekly-breakdown/week5/index.mdx","sourceDirName":"weekly-breakdown/week5","slug":"/weekly-breakdown/week5/","permalink":"/physical-ai-book/docs/weekly-breakdown/week5/","draft":false,"unlisted":false,"editUrl":"https://github.com/fatima317/physical-ai-book/tree/main/docs/weekly-breakdown/week5/index.mdx","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Week 5 - Computer Vision for Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Week 4 - Navigation and Path Planning","permalink":"/physical-ai-book/docs/weekly-breakdown/week4/"},"next":{"title":"Week 6 - Machine Learning for Robotics","permalink":"/physical-ai-book/docs/weekly-breakdown/week6/"}}');var s=i(4848),r=i(8453);const o={sidebar_position:5,title:"Week 5 - Computer Vision for Robotics"},a="Week 5 - Computer Vision for Robotics",c={},l=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Computer Vision Fundamentals",id:"computer-vision-fundamentals",level:2},{value:"Code Snippets",id:"code-snippets",level:2},{value:"Basic Image Processing with OpenCV",id:"basic-image-processing-with-opencv",level:3},{value:"Feature Detection and Matching",id:"feature-detection-and-matching",level:3},{value:"Object Detection with YOLO",id:"object-detection-with-yolo",level:3},{value:"URDF Examples",id:"urdf-examples",level:2},{value:"Vision-enabled Robot",id:"vision-enabled-robot",level:3},{value:"Computer Vision Pipeline Diagram",id:"computer-vision-pipeline-diagram",level:2},{value:"Vision Algorithms Comparison",id:"vision-algorithms-comparison",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Learning Checkpoints",id:"learning-checkpoints",level:2},{value:"Quiz Questions",id:"quiz-questions",level:3},{value:"Practical Exercise",id:"practical-exercise",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Personalization",id:"personalization",level:2},{value:"Translation",id:"translation",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"week-5---computer-vision-for-robotics",children:"Week 5 - Computer Vision for Robotics"})}),"\n",(0,s.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Implement basic computer vision algorithms for robotics applications"}),"\n",(0,s.jsx)(n.li,{children:"Use OpenCV for image processing and feature extraction"}),"\n",(0,s.jsx)(n.li,{children:"Apply object detection and tracking techniques"}),"\n",(0,s.jsx)(n.li,{children:"Integrate vision systems with robot control"}),"\n",(0,s.jsx)(n.li,{children:"Perform stereo vision and depth estimation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"computer-vision-fundamentals",children:"Computer Vision Fundamentals"}),"\n",(0,s.jsx)(n.p,{children:"Computer vision in robotics involves processing visual information to understand and interact with the environment. Key components include:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Acquisition"}),": Capturing images from cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Preprocessing"}),": Enhancing image quality and reducing noise"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Extraction"}),": Identifying distinctive elements in images"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Object Recognition"}),": Detecting and classifying objects"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Tracking"}),": Following objects across multiple frames"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"code-snippets",children:"Code Snippets"}),"\n",(0,s.jsx)(n.h3,{id:"basic-image-processing-with-opencv",children:"Basic Image Processing with OpenCV"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\n\nclass VisionNode(Node):\n    def __init__(self):\n        super().__init__(\'vision_node\')\n\n        # Initialize CV bridge\n        self.bridge = CvBridge()\n\n        # Create subscription to camera feed\n        self.subscription = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n\n        # Create publisher for processed image\n        self.publisher = self.create_publisher(\n            Image,\n            \'/camera/processed_image\',\n            10\n        )\n\n        # Processed image counter\n        self.processed_count = 0\n\n    def image_callback(self, msg):\n        """Process incoming image"""\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\n\n            # Apply basic image processing\n            processed_image = self.process_image(cv_image)\n\n            # Convert back to ROS Image\n            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, "bgr8")\n            processed_msg.header = msg.header\n\n            # Publish processed image\n            self.publisher.publish(processed_msg)\n\n            self.processed_count += 1\n            if self.processed_count % 30 == 0:  # Log every 30 frames\n                self.get_logger().info(f\'Processed {self.processed_count} images\')\n\n        except Exception as e:\n            self.get_logger().error(f\'Error processing image: {e}\')\n\n    def process_image(self, image):\n        """Apply image processing techniques"""\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Apply Gaussian blur to reduce noise\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n\n        # Apply threshold\n        _, thresh = cv2.threshold(blurred, 127, 255, cv2.THRESH_BINARY)\n\n        # Apply morphological operations to clean up\n        kernel = np.ones((3, 3), np.uint8)\n        opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n\n        # Find contours\n        contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n        # Draw contours on original image\n        result = image.copy()\n        cv2.drawContours(result, contours, -1, (0, 255, 0), 2)\n\n        return result\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vision_node = VisionNode()\n\n    try:\n        rclpy.spin(vision_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        vision_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h3,{id:"feature-detection-and-matching",children:"Feature Detection and Matching"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import cv2\nimport numpy as np\n\nclass FeatureDetector:\n    def __init__(self):\n        # Initialize SIFT detector\n        self.detector = cv2.SIFT_create()\n        # Initialize matcher\n        self.matcher = cv2.BFMatcher()\n\n    def detect_and_describe(self, image):\n        """Detect features and compute descriptors"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        keypoints, descriptors = self.detector.detectAndCompute(gray, None)\n        return keypoints, descriptors\n\n    def match_features(self, desc1, desc2):\n        """Match features between two images"""\n        if desc1 is None or desc2 is None:\n            return []\n\n        # Use KNN matching\n        matches = self.matcher.knnMatch(desc1, desc2, k=2)\n\n        # Apply Lowe\'s ratio test\n        good_matches = []\n        for m, n in matches:\n            if m.distance < 0.75 * n.distance:\n                good_matches.append(m)\n\n        return good_matches\n\n    def find_object(self, scene_image, template_image):\n        """Find template in scene using feature matching"""\n        # Detect features in both images\n        kp1, desc1 = self.detect_and_describe(template_image)\n        kp2, desc2 = self.detect_and_describe(scene_image)\n\n        if desc1 is None or desc2 is None:\n            return None, None\n\n        # Match features\n        matches = self.match_features(desc1, desc2)\n\n        if len(matches) >= 10:\n            # Extract matched keypoints\n            src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n            dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n\n            # Find homography matrix\n            M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n\n            if M is not None:\n                # Define template corners\n                h, w = template_image.shape[:2]\n                corners = np.float32([\n                    [0, 0], [w, 0], [w, h], [0, h]\n                ]).reshape(-1, 1, 2)\n\n                # Transform corners to scene\n                transformed_corners = cv2.perspectiveTransform(corners, M)\n\n                return transformed_corners, matches\n        else:\n            print(f"Not enough matches found: {len(matches)}/{10}")\n\n        return None, None\n\n# Example usage\ndef example_feature_detection():\n    # Load images\n    template = cv2.imread(\'template.jpg\')\n    scene = cv2.imread(\'scene.jpg\')\n\n    if template is None or scene is None:\n        print("Could not load images")\n        return\n\n    detector = FeatureDetector()\n    corners, matches = detector.find_object(scene, template)\n\n    if corners is not None:\n        # Draw rectangle around detected object\n        result = scene.copy()\n        cv2.polylines(result, [np.int32(corners)], True, (0, 255, 0), 3, cv2.LINE_AA)\n        cv2.imshow(\'Detected Object\', result)\n        cv2.waitKey(0)\n        cv2.destroyAllWindows()\n    else:\n        print("Object not found in scene")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"object-detection-with-yolo",children:"Object Detection with YOLO"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\nclass YOLODetector:\n    def __init__(self, config_path, weights_path, names_path):\n        # Load YOLO network\n        self.net = cv2.dnn.readNet(weights_path, config_path)\n\n        # Load class names\n        with open(names_path, 'r') as f:\n            self.classes = [line.strip() for line in f.readlines()]\n\n        # Get output layers\n        layer_names = self.net.getLayerNames()\n        self.output_layers = [layer_names[i[0] - 1] for i in self.net.getUnconnectedOutLayers()]\n\n        # Set backend and target\n        self.net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)\n\n    def detect_objects(self, image, confidence_threshold=0.5):\n        \"\"\"Detect objects in image using YOLO\"\"\"\n        height, width, channels = image.shape\n\n        # Prepare image for YOLO\n        blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n        self.net.setInput(blob)\n        outputs = self.net.forward(self.output_layers)\n\n        # Process outputs\n        boxes = []\n        confidences = []\n        class_ids = []\n\n        for output in outputs:\n            for detection in output:\n                scores = detection[5:]\n                class_id = np.argmax(scores)\n                confidence = scores[class_id]\n\n                if confidence > confidence_threshold:\n                    # Object detected\n                    center_x = int(detection[0] * width)\n                    center_y = int(detection[1] * height)\n                    w = int(detection[2] * width)\n                    h = int(detection[3] * height)\n\n                    # Rectangle coordinates\n                    x = int(center_x - w / 2)\n                    y = int(center_y - h / 2)\n\n                    boxes.append([x, y, w, h])\n                    confidences.append(float(confidence))\n                    class_ids.append(class_id)\n\n        # Apply non-maximum suppression\n        indexes = cv2.dnn.NMSBoxes(boxes, confidences, confidence_threshold, 0.4)\n\n        # Format results\n        detections = []\n        if len(indexes) > 0:\n            for i in indexes.flatten():\n                x, y, w, h = boxes[i]\n                class_name = self.classes[class_ids[i]]\n                confidence = confidences[i]\n                detections.append({\n                    'bbox': (x, y, w, h),\n                    'class': class_name,\n                    'confidence': confidence\n                })\n\n        return detections\n\n    def draw_detections(self, image, detections):\n        \"\"\"Draw bounding boxes on image\"\"\"\n        colors = np.random.uniform(0, 255, size=(len(self.classes), 3))\n\n        for detection in detections:\n            x, y, w, h = detection['bbox']\n            class_name = detection['class']\n            confidence = detection['confidence']\n\n            # Draw bounding box\n            color = colors[list(self.classes).index(class_name)]\n            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n\n            # Draw label\n            label = f\"{class_name}: {confidence:.2f}\"\n            cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n\n        return image\n"})}),"\n",(0,s.jsx)(n.h2,{id:"urdf-examples",children:"URDF Examples"}),"\n",(0,s.jsx)(n.h3,{id:"vision-enabled-robot",children:"Vision-enabled Robot"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="vision_robot">\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10.0"/>\n      <inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Wheels --\x3e\n  <joint name="wheel_left_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_left"/>\n    <origin xyz="0 0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="wheel_left">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  <joint name="wheel_right_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_right"/>\n    <origin xyz="0 -0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="wheel_right">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Camera Mount --\x3e\n  <joint name="camera_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.2 0.0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Depth Camera Mount --\x3e\n  <joint name="depth_camera_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="depth_camera_link"/>\n    <origin xyz="0.2 0.0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="depth_camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0 0 1 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Vision Sensors --\x3e\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>camera_link</frame_name>\n        <topic_name>camera/image_raw</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="depth_camera_link">\n    <sensor name="depth_camera" type="depth">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n      <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n        <frame_name>depth_camera_link</frame_name>\n        <depth_image_topic_name>depth/image_raw</depth_image_topic_name>\n        <point_cloud_topic_name>depth/points</point_cloud_topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Differential Drive Controller --\x3e\n  <gazebo>\n    <plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">\n      <left_joint>wheel_left_joint</left_joint>\n      <right_joint>wheel_right_joint</right_joint>\n      <wheel_separation>0.5</wheel_separation>\n      <wheel_diameter>0.2</wheel_diameter>\n      <command_topic>cmd_vel</command_topic>\n      <odometry_topic>odom</odometry_topic>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_link</robot_base_frame>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,s.jsx)(n.h2,{id:"computer-vision-pipeline-diagram",children:"Computer Vision Pipeline Diagram"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Camera Input] --\x3e B[Image Preprocessing]\n    B --\x3e C[Feature Extraction]\n    C --\x3e D[Object Detection]\n    D --\x3e E[Object Tracking]\n    E --\x3e F[Scene Understanding]\n    F --\x3e G[Robot Control]\n    G --\x3e H[Action Execution]\n    H --\x3e I[Feedback Loop]\n    I --\x3e B\n\n    style A fill:#ff9999\n    style D fill:#99ff99\n    style G fill:#99ccff\n"})}),"\n",(0,s.jsx)(n.h2,{id:"vision-algorithms-comparison",children:"Vision Algorithms Comparison"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Algorithm"}),(0,s.jsx)(n.th,{children:"Purpose"}),(0,s.jsx)(n.th,{children:"Speed"}),(0,s.jsx)(n.th,{children:"Accuracy"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"SIFT"}),(0,s.jsx)(n.td,{children:"Feature Detection"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Object Recognition"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"ORB"}),(0,s.jsx)(n.td,{children:"Feature Detection"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Real-time Applications"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"YOLO"}),(0,s.jsx)(n.td,{children:"Object Detection"}),(0,s.jsx)(n.td,{children:"Very Fast"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"Real-time Detection"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"HOG+SVM"}),(0,s.jsx)(n.td,{children:"Pedestrian Detection"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Safety Applications"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Optical Flow"}),(0,s.jsx)(n.td,{children:"Motion Tracking"}),(0,s.jsx)(n.td,{children:"Fast"}),(0,s.jsx)(n.td,{children:"Medium"}),(0,s.jsx)(n.td,{children:"Movement Analysis"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Stereo Vision"}),(0,s.jsx)(n.td,{children:"Depth Estimation"}),(0,s.jsx)(n.td,{children:"Slow"}),(0,s.jsx)(n.td,{children:"High"}),(0,s.jsx)(n.td,{children:"3D Reconstruction"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenCV"}),": Open Computer Vision library for image processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Feature Detection"}),": Finding distinctive points in an image"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Homography"}),": Transformation between two planes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Vision"}),": Depth estimation using two cameras"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optical Flow"}),": Motion of objects between frames"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Perspective Transform"}),": Correcting perspective distortion"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Image Segmentation"}),": Partitioning image into regions"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"learning-checkpoints",children:"Learning Checkpoints"}),"\n",(0,s.jsx)(n.h3,{id:"quiz-questions",children:"Quiz Questions"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"What is the main advantage of SIFT over ORB for feature detection?"}),"\n",(0,s.jsx)(n.li,{children:"Name three different approaches for object detection in robotics."}),"\n",(0,s.jsx)(n.li,{children:"How does stereo vision enable depth estimation?"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,s.jsx)(n.p,{children:"Implement a simple object detection system that can identify and track a colored ball in real-time video feed."}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,s.jsx)(n.p,{children:"Create a ROS 2 node that subscribes to camera data, detects a specific object (e.g., a colored ball), and publishes commands to drive the robot toward the object."}),"\n",(0,s.jsx)(n.h2,{id:"personalization",children:"Personalization"}),"\n",(0,s.jsxs)("div",{className:"personalization-options",children:[(0,s.jsx)("h3",{children:"Adjust Learning Path:"}),(0,s.jsx)("button",{onClick:()=>setDifficulty("beginner"),children:"Beginner"}),(0,s.jsx)("button",{onClick:()=>setDifficulty("intermediate"),children:"Intermediate"}),(0,s.jsx)("button",{onClick:()=>setDifficulty("advanced"),children:"Advanced"})]}),"\n",(0,s.jsx)(n.h2,{id:"translation",children:"Translation"}),"\n",(0,s.jsx)("div",{className:"translation-controls",children:(0,s.jsx)("button",{onClick:()=>translateToUrdu(),children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u062a\u0631\u062c\u0645\u06c1 \u06a9\u0631\u06cc\u06ba"})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const s={},r=t.createContext(s);function o(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);