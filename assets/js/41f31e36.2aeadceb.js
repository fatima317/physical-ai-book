"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[4938],{7750:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"weekly-breakdown/week10/index","title":"Week 10 - Perception and Sensor Fusion","description":"Learning Objectives","source":"@site/docs/weekly-breakdown/week10/index.mdx","sourceDirName":"weekly-breakdown/week10","slug":"/weekly-breakdown/week10/","permalink":"/physical-ai-book/docs/weekly-breakdown/week10/","draft":false,"unlisted":false,"editUrl":"https://github.com/fatima317/physical-ai-book/tree/main/docs/weekly-breakdown/week10/index.mdx","tags":[],"version":"current","sidebarPosition":10,"frontMatter":{"sidebar_position":10,"title":"Week 10 - Perception and Sensor Fusion"},"sidebar":"tutorialSidebar","previous":{"title":"Week 9 - Control Systems for Robotics","permalink":"/physical-ai-book/docs/weekly-breakdown/week9/"},"next":{"title":"Week 11 - AI Planning and Decision Making","permalink":"/physical-ai-book/docs/weekly-breakdown/week11/"}}');var a=i(4848),s=i(8453);const r={sidebar_position:10,title:"Week 10 - Perception and Sensor Fusion"},o="Week 10 - Perception and Sensor Fusion",l={},m=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Sensor Fusion Fundamentals",id:"sensor-fusion-fundamentals",level:2},{value:"Code Snippets",id:"code-snippets",level:2},{value:"Kalman Filter Implementation",id:"kalman-filter-implementation",level:3},{value:"Particle Filter Implementation",id:"particle-filter-implementation",level:3},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:3},{value:"URDF Examples",id:"urdf-examples",level:2},{value:"Multi-Sensor Robot Platform",id:"multi-sensor-robot-platform",level:3},{value:"Sensor Fusion Pipeline Diagram",id:"sensor-fusion-pipeline-diagram",level:2},{value:"Sensor Comparison",id:"sensor-comparison",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Learning Checkpoints",id:"learning-checkpoints",level:2},{value:"Quiz Questions",id:"quiz-questions",level:3},{value:"Practical Exercise",id:"practical-exercise",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Personalization",id:"personalization",level:2},{value:"Translation",id:"translation",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-10---perception-and-sensor-fusion",children:"Week 10 - Perception and Sensor Fusion"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Implement sensor fusion algorithms for robust perception"}),"\n",(0,a.jsx)(n.li,{children:"Apply Kalman filtering for state estimation"}),"\n",(0,a.jsx)(n.li,{children:"Integrate multiple sensor modalities (LIDAR, camera, IMU)"}),"\n",(0,a.jsx)(n.li,{children:"Implement SLAM (Simultaneous Localization and Mapping)"}),"\n",(0,a.jsx)(n.li,{children:"Design robust perception systems for uncertain environments"}),"\n",(0,a.jsx)(n.li,{children:"Evaluate sensor fusion performance metrics"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-fundamentals",children:"Sensor Fusion Fundamentals"}),"\n",(0,a.jsx)(n.p,{children:"Sensor fusion combines data from multiple sensors to achieve better accuracy and reliability than individual sensors alone. Key benefits include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Redundancy"}),": Backup measurements if one sensor fails"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Complementarity"}),": Different sensors provide complementary information"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Improved Accuracy"}),": Combined measurements are more accurate"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Robustness"}),": System continues operating despite sensor noise/errors"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code-snippets",children:"Code Snippets"}),"\n",(0,a.jsx)(n.h3,{id:"kalman-filter-implementation",children:"Kalman Filter Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.linalg import inv\n\nclass KalmanFilter:\n    def __init__(self, state_dim, measurement_dim):\n        """\n        Initialize Kalman Filter\n\n        Args:\n            state_dim: Dimension of state vector\n            measurement_dim: Dimension of measurement vector\n        """\n        self.state_dim = state_dim\n        self.measurement_dim = measurement_dim\n\n        # State vector [x, y, vx, vy] for 2D position and velocity\n        self.x = np.zeros(state_dim)\n\n        # State covariance matrix\n        self.P = np.eye(state_dim) * 1000.0  # Initial uncertainty\n\n        # Process noise covariance\n        self.Q = np.eye(state_dim) * 0.1\n\n        # Measurement noise covariance\n        self.R = np.eye(measurement_dim) * 1.0\n\n        # State transition model\n        self.F = np.eye(state_dim)\n\n        # Measurement model\n        self.H = np.zeros((measurement_dim, state_dim))\n\n    def predict(self, dt=1.0):\n        """\n        Predict next state using motion model\n        For constant velocity model: x_{k+1} = F * x_k\n        """\n        # Update state transition matrix for constant velocity model\n        # State: [x, y, vx, vy]\n        self.F = np.array([\n            [1, 0, dt, 0],\n            [0, 1, 0, dt],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1]\n        ])\n\n        # Predict state: x = F * x\n        self.x = self.F @ self.x\n\n        # Predict covariance: P = F * P * F^T + Q\n        self.P = self.F @ self.P @ self.F.T + self.Q\n\n    def update(self, measurement):\n        """\n        Update state estimate with new measurement\n\n        Args:\n            measurement: Observed measurement vector\n        """\n        # Innovation: y = z - H * x\n        innovation = measurement - self.H @ self.x\n\n        # Innovation covariance: S = H * P * H^T + R\n        S = self.H @ self.P @ self.H.T + self.R\n\n        # Kalman gain: K = P * H^T * S^(-1)\n        K = self.P @ self.H.T @ inv(S)\n\n        # Update state: x = x + K * y\n        self.x = self.x + K @ innovation\n\n        # Update covariance: P = (I - K * H) * P\n        I = np.eye(self.state_dim)\n        self.P = (I - K @ self.H) @ self.P\n\nclass ExtendedKalmanFilter(KalmanFilter):\n    """\n    Extended Kalman Filter for nonlinear systems\n    """\n    def __init__(self, state_dim, measurement_dim):\n        super().__init__(state_dim, measurement_dim)\n\n    def predict(self, dt=1.0):\n        """Nonlinear prediction step"""\n        # For nonlinear systems, we linearize around current state\n        # This example uses constant velocity model (linear)\n        # In practice, you\'d implement nonlinear motion model\n        super().predict(dt)\n\n    def update(self, measurement):\n        """Nonlinear update step"""\n        # For EKF, we need to compute Jacobians of nonlinear measurement model\n        # This example assumes linear measurement model\n        super().update(measurement)\n\nclass SensorFusionNode:\n    """\n    ROS 2 node for sensor fusion\n    """\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from sensor_msgs.msg import Imu, LaserScan, PointCloud2\n        from geometry_msgs.msg import PoseWithCovarianceStamped\n        from tf2_ros import TransformBroadcaster\n\n        self.node = Node(\'sensor_fusion_node\')\n\n        # Initialize Kalman filter for position and velocity estimation\n        # State: [x, y, theta, vx, vy, omega]\n        self.kf = KalmanFilter(state_dim=6, measurement_dim=3)  # x, y, theta\n\n        # Measurement models for different sensors\n        # IMU: measures orientation and angular velocity\n        self.kf.H_imu = np.array([\n            [0, 0, 1, 0, 0, 0],  # theta\n            [0, 0, 0, 0, 0, 1]   # omega (angular velocity)\n        ])\n\n        # LIDAR: measures position\n        self.kf.H_lidar = np.array([\n            [1, 0, 0, 0, 0, 0],  # x\n            [0, 1, 0, 0, 0, 0]   # y\n        ])\n\n        # Initialize sensor data\n        self.imu_data = None\n        self.lidar_data = None\n        self.odom_data = None\n\n        # Timers for fusion updates\n        self.fusion_timer = self.node.create_timer(0.05, self.perform_fusion)  # 20 Hz\n\n        # Publishers and subscribers\n        self.pose_pub = self.node.create_publisher(PoseWithCovarianceStamped, \'estimated_pose\', 10)\n        self.tf_broadcaster = TransformBroadcaster(self.node)\n\n        # Sensor subscribers\n        self.imu_sub = self.node.create_subscription(Imu, \'imu/data\', self.imu_callback, 10)\n        self.scan_sub = self.node.create_subscription(LaserScan, \'scan\', self.lidar_callback, 10)\n\n        self.node.get_logger().info("Sensor fusion node initialized")\n\n    def imu_callback(self, msg):\n        """Handle IMU data"""\n        # Extract orientation (convert from quaternion to euler)\n        import quaternion\n        quat = np.array([msg.orientation.w, msg.orientation.x, msg.orientation.y, msg.orientation.z])\n        r = quaternion.from_float_array(quat)\n        euler = quaternion.as_euler_angles(r)\n\n        # Extract angular velocity\n        omega = msg.angular_velocity.z  # Around z-axis\n\n        # Store IMU measurement: [theta, omega]\n        self.imu_data = np.array([euler[2], omega])  # Yaw angle and angular velocity\n\n    def lidar_callback(self, msg):\n        """Handle LIDAR data"""\n        # For simplicity, assume we get position from LIDAR\n        # In practice, this would involve landmark detection or SLAM\n        # This is a simplified example\n        pass\n\n    def perform_fusion(self):\n        """Perform sensor fusion and publish results"""\n        # Prediction step (use motion model if available)\n        # For now, assume dt = 0.05 seconds\n        dt = 0.05\n        self.kf.predict(dt)\n\n        # Update with available measurements\n        if self.imu_data is not None:\n            # Use appropriate measurement model for IMU\n            # Update Kalman filter with IMU data\n            pass\n\n        if self.lidar_data is not None:\n            # Update with LIDAR data\n            pass\n\n        # Publish estimated state\n        self.publish_estimate()\n\n    def publish_estimate(self):\n        """Publish estimated pose"""\n        from geometry_msgs.msg import PoseWithCovarianceStamped, Point, Quaternion\n        from std_msgs.msg import Header\n\n        msg = PoseWithCovarianceStamped()\n        msg.header.stamp = self.node.get_clock().now().to_msg()\n        msg.header.frame_id = \'map\'\n\n        # Extract position and orientation from state\n        x, y, theta, vx, vy, omega = self.kf.x\n\n        # Set position\n        msg.pose.pose.position.x = float(x)\n        msg.pose.pose.position.y = float(y)\n        msg.pose.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        from mathutils import quaternion_from_euler\n        qw, qx, qy, qz = quaternion_from_euler(0, 0, theta)\n        msg.pose.pose.orientation.w = qw\n        msg.pose.pose.orientation.x = qx\n        msg.pose.pose.orientation.y = qy\n        msg.pose.pose.orientation.z = qz\n\n        # Set covariance\n        for i in range(36):\n            msg.pose.covariance[i] = float(self.kf.P[i//6, i%6])\n\n        self.pose_pub.publish(msg)\n\ndef quaternion_from_euler(roll, pitch, yaw):\n    """Convert Euler angles to quaternion"""\n    cy = np.cos(yaw * 0.5)\n    sy = np.sin(yaw * 0.5)\n    cp = np.cos(pitch * 0.5)\n    sp = np.sin(pitch * 0.5)\n    cr = np.cos(roll * 0.5)\n    sr = np.sin(roll * 0.5)\n\n    w = cr * cp * cy + sr * sp * sy\n    x = sr * cp * cy - cr * sp * sy\n    y = cr * sp * cy + sr * cp * sy\n    z = cr * cp * sy - sr * sp * cy\n\n    return w, x, y, z\n'})}),"\n",(0,a.jsx)(n.h3,{id:"particle-filter-implementation",children:"Particle Filter Implementation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom scipy.stats import norm\nfrom scipy.spatial.distance import cdist\n\nclass ParticleFilter:\n    def __init__(self, num_particles=1000, state_dim=2):\n        """\n        Initialize Particle Filter\n\n        Args:\n            num_particles: Number of particles to use\n            state_dim: Dimension of state vector\n        """\n        self.num_particles = num_particles\n        self.state_dim = state_dim\n\n        # Initialize particles randomly\n        self.particles = np.random.randn(num_particles, state_dim) * 10\n        self.weights = np.ones(num_particles) / num_particles\n\n    def predict(self, motion_model_func, control_input, process_noise_std=0.1):\n        """\n        Predict particle states using motion model\n\n        Args:\n            motion_model_func: Function that takes (particles, control_input) and returns new particles\n            control_input: Control input for motion model\n            process_noise_std: Standard deviation of process noise\n        """\n        # Apply motion model\n        self.particles = motion_model_func(self.particles, control_input)\n\n        # Add process noise\n        noise = np.random.normal(0, process_noise_std, self.particles.shape)\n        self.particles += noise\n\n    def update(self, measurement, measurement_model_func, measurement_noise_std=0.1):\n        """\n        Update particle weights based on measurement\n\n        Args:\n            measurement: Observed measurement\n            measurement_model_func: Function that maps state to expected measurement\n            measurement_noise_std: Standard deviation of measurement noise\n        """\n        # Calculate expected measurements for each particle\n        expected_measurements = measurement_model_func(self.particles)\n\n        # Calculate likelihood of measurement for each particle\n        # Using Gaussian likelihood\n        likelihoods = norm.pdf(measurement, expected_measurements, measurement_noise_std)\n\n        # Update weights\n        self.weights *= likelihoods\n\n        # Normalize weights\n        self.weights /= np.sum(self.weights)\n\n        # Prevent numerical issues\n        self.weights = np.maximum(self.weights, 1e-300)\n\n    def resample(self):\n        """Resample particles based on weights"""\n        # Systematic resampling\n        indices = self.systematic_resample()\n\n        # Resample particles and reset weights\n        self.particles = self.particles[indices]\n        self.weights = np.ones(self.num_particles) / self.num_particles\n\n    def systematic_resample(self):\n        """Systematic resampling algorithm"""\n        # Cumulative sum of weights\n        cumulative_sum = np.cumsum(self.weights)\n\n        # Generate random starting point\n        start = np.random.uniform(0, 1/self.num_particles)\n        indices = []\n\n        # Generate equally spaced points\n        points = start + (np.arange(self.num_particles) / self.num_particles)\n\n        # Find indices of particles to resample\n        i, j = 0, 0\n        while i < self.num_particles:\n            if points[i] < cumulative_sum[j]:\n                indices.append(j)\n                i += 1\n            else:\n                j += 1\n\n        return np.array(indices)\n\n    def estimate(self):\n        """Estimate state as weighted average of particles"""\n        return np.average(self.particles, axis=0, weights=self.weights)\n\n    def motion_model(self, particles, control):\n        """\n        Simple motion model: x_{t+1} = x_t + v*dt + noise\n        Assumes control = [vx, vy, dt]\n        """\n        dt = control[2] if len(control) > 2 else 0.1\n        velocity = control[:2] if len(control) >= 2 else np.array([0.0, 0.0])\n\n        # Update positions: x_new = x_old + v * dt\n        updated_particles = particles.copy()\n        updated_particles[:, :2] += velocity * dt\n\n        return updated_particles\n\n    def measurement_model(self, particles):\n        """Measurement model: return expected measurements for particles"""\n        # For this example, assume we measure position directly\n        return particles[:, :2]  # Return x, y coordinates\n\n# Example usage\ndef example_particle_filter():\n    # Initialize particle filter\n    pf = ParticleFilter(num_particles=1000, state_dim=4)  # [x, y, vx, vy]\n\n    # Simulate a moving target\n    true_state = np.array([0.0, 0.0, 0.1, 0.05])  # [x, y, vx, vy]\n\n    measurements = []\n    estimates = []\n\n    for t in np.arange(0, 10, 0.1):  # 10 seconds at 10 Hz\n        dt = 0.1\n\n        # Simulate motion\n        true_state[:2] += true_state[2:] * dt  # Update position\n\n        # Add some process noise\n        true_state += np.random.normal(0, 0.01, 4)\n\n        # Generate noisy measurement\n        measurement = true_state[:2] + np.random.normal(0, 0.1, 2)  # Only measure position\n        measurements.append(measurement.copy())\n\n        # Perform particle filter steps\n        control = np.array([0.1, 0.05, dt])  # [vx, vy, dt]\n\n        pf.predict(pf.motion_model, control)\n        pf.update(measurement, pf.measurement_model)\n        pf.resample()\n\n        estimate = pf.estimate()\n        estimates.append(estimate.copy())\n\n        if t % 1 == 0:  # Print every second\n            print(f"Time {t:.1f}: True={true_state[:2]}, Measured={measurement}, Estimated={estimate[:2]}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nfrom collections import deque\nimport threading\nimport time\n\nclass MultiSensorFusion:\n    def __init__(self):\n        # Sensor data buffers\n        self.imu_buffer = deque(maxlen=10)\n        self.lidar_buffer = deque(maxlen=10)\n        self.camera_buffer = deque(maxlen=10)\n        self.odom_buffer = deque(maxlen=10)\n\n        # Fusion state\n        self.position = np.array([0.0, 0.0, 0.0])  # x, y, z\n        self.velocity = np.array([0.0, 0.0, 0.0])  # vx, vy, vz\n        self.orientation = np.array([0.0, 0.0, 0.0, 1.0])  # x, y, z, w (quaternion)\n        self.timestamp = time.time()\n\n        # Sensor uncertainties\n        self.imu_uncertainty = 0.01\n        self.lidar_uncertainty = 0.05\n        self.camera_uncertainty = 0.03\n        self.odom_uncertainty = 0.02\n\n        # Lock for thread safety\n        self.lock = threading.Lock()\n\n    def add_imu_data(self, accel, gyro, timestamp):\n        """Add IMU data to buffer"""\n        with self.lock:\n            self.imu_buffer.append({\n                \'acceleration\': np.array(accel),\n                \'gyro\': np.array(gyro),\n                \'timestamp\': timestamp\n            })\n\n    def add_lidar_data(self, ranges, angles, timestamp):\n        """Add LIDAR data to buffer"""\n        with self.lock:\n            self.lidar_buffer.append({\n                \'ranges\': np.array(ranges),\n                \'angles\': np.array(angles),\n                \'timestamp\': timestamp\n            })\n\n    def add_camera_data(self, features, timestamp):\n        """Add camera data to buffer"""\n        with self.lock:\n            self.camera_buffer.append({\n                \'features\': features,\n                \'timestamp\': timestamp\n            })\n\n    def add_odom_data(self, pose, twist, timestamp):\n        """Add odometry data to buffer"""\n        with self.lock:\n            self.odom_buffer.append({\n                \'pose\': pose,\n                \'twist\': twist,\n                \'timestamp\': timestamp\n            })\n\n    def fuse_sensors(self):\n        """Perform sensor fusion to estimate state"""\n        with self.lock:\n            # Get latest data from all sensors\n            latest_imu = self.get_latest_data(self.imu_buffer)\n            latest_lidar = self.get_latest_data(self.lidar_buffer)\n            latest_camera = self.get_latest_data(self.camera_buffer)\n            latest_odom = self.get_latest_data(self.odom_buffer)\n\n            # Fuse data using weighted average based on uncertainties\n            if latest_odom:\n                # Odometry provides good position estimate\n                pos_weight = 1.0 / self.odom_uncertainty\n                self.position = latest_odom[\'pose\'][\'position\'] * pos_weight\n\n            if latest_imu:\n                # IMU provides good orientation and acceleration\n                self.orientation = latest_imu[\'orientation\']\n\n                # Integrate acceleration to get velocity\n                if self.imu_buffer:\n                    dt = 0.01  # Assume 100Hz IMU\n                    self.velocity += latest_imu[\'acceleration\'] * dt\n\n            # Apply sensor fusion weights based on reliability\n            # This is a simplified example - in practice, you\'d use Kalman filtering\n            self.apply_sensor_weights(latest_imu, latest_lidar, latest_camera, latest_odom)\n\n    def get_latest_data(self, buffer):\n        """Get the latest data from buffer"""\n        if buffer:\n            return buffer[-1]\n        return None\n\n    def apply_sensor_weights(self, imu_data, lidar_data, camera_data, odom_data):\n        """Apply weighted fusion based on sensor reliability"""\n        weights = []\n        estimates = []\n\n        # Position estimates from different sensors\n        if odom_data:\n            weights.append(1.0 / self.odom_uncertainty)\n            estimates.append(odom_data[\'pose\'][\'position\'])\n\n        if lidar_data:\n            # Process LIDAR data to get position estimate\n            # This is simplified - in practice, you\'d do landmark matching\n            lidar_pos = self.process_lidar_position(lidar_data)\n            if lidar_pos is not None:\n                weights.append(1.0 / self.lidar_uncertainty)\n                estimates.append(lidar_pos)\n\n        if camera_data:\n            # Process camera data to get position estimate\n            camera_pos = self.process_camera_position(camera_data)\n            if camera_pos is not None:\n                weights.append(1.0 / self.camera_uncertainty)\n                estimates.append(camera_pos)\n\n        # Weighted average of position estimates\n        if estimates and weights:\n            weights = np.array(weights)\n            estimates = np.array(estimates)\n\n            # Normalize weights\n            weights = weights / np.sum(weights)\n\n            # Calculate weighted average\n            fused_position = np.average(estimates, axis=0, weights=weights)\n\n            # Update position with fused estimate\n            self.position = fused_position\n\n    def process_lidar_position(self, lidar_data):\n        """Process LIDAR data to estimate position relative to landmarks"""\n        # This is a simplified approach\n        # In practice, you\'d use landmark matching or SLAM\n        ranges = lidar_data[\'ranges\']\n        angles = lidar_data[\'angles\']\n\n        # Find closest obstacle to estimate proximity\n        if len(ranges) > 0:\n            min_idx = np.argmin(ranges)\n            min_range = ranges[min_idx]\n            min_angle = angles[min_idx]\n\n            # Convert polar to cartesian\n            x_obs = min_range * np.cos(min_angle)\n            y_obs = min_range * np.sin(min_angle)\n\n            # Return observation (would need landmark map to convert to absolute position)\n            return np.array([x_obs, y_obs, 0.0])\n\n        return None\n\n    def process_camera_position(self, camera_data):\n        """Process camera data to estimate position"""\n        # This is a simplified approach\n        # In practice, you\'d use feature tracking or visual odometry\n        features = camera_data[\'features\']\n\n        # For this example, return a dummy position based on features\n        # In reality, you\'d track features across frames\n        if features:\n            # Calculate average feature position as proxy for camera position\n            avg_x = np.mean([f[\'x\'] for f in features if \'x\' in f])\n            avg_y = np.mean([f[\'y\'] for f in features if \'y\' in f])\n            return np.array([avg_x, avg_y, 0.0])\n\n        return None\n\nclass SLAMNode:\n    """\n    Simultaneous Localization and Mapping node\n    """\n    def __init__(self):\n        # Landmark map (in a real system, this would be much more complex)\n        self.landmarks = {}  # Dictionary: landmark_id -> [x, y]\n        self.pose_graph = []  # Graph of poses and constraints\n\n        # Covariance for landmark positions\n        self.landmark_covariances = {}\n\n    def add_landmark_observation(self, landmark_id, relative_pose, sensor_uncertainty=0.1):\n        """\n        Add landmark observation to map\n\n        Args:\n            landmark_id: Unique identifier for landmark\n            relative_pose: Position of landmark relative to robot [x, y]\n            sensor_uncertainty: Uncertainty in sensor measurement\n        """\n        # Get current robot pose (this would come from localization)\n        robot_pose = np.array([0.0, 0.0, 0.0])  # [x, y, theta]\n\n        # Convert relative pose to global coordinates\n        cos_th = np.cos(robot_pose[2])\n        sin_th = np.sin(robot_pose[2])\n\n        global_x = robot_pose[0] + relative_pose[0] * cos_th - relative_pose[1] * sin_th\n        global_y = robot_pose[1] + relative_pose[0] * sin_th + relative_pose[1] * cos_th\n\n        # Update landmark position estimate\n        if landmark_id in self.landmarks:\n            # Fuse with existing estimate\n            old_pos = self.landmarks[landmark_id]\n            old_cov = self.landmark_covariances[landmark_id]\n\n            # Simple weighted fusion\n            weight_old = 1.0 / (old_cov + 1e-6)\n            weight_new = 1.0 / (sensor_uncertainty**2)\n\n            new_pos = (weight_old * old_pos + weight_new * np.array([global_x, global_y])) / (weight_old + weight_new)\n            new_cov = 1.0 / (weight_old + weight_new)\n\n            self.landmarks[landmark_id] = new_pos\n            self.landmark_covariances[landmark_id] = new_cov\n        else:\n            # Add new landmark\n            self.landmarks[landmark_id] = np.array([global_x, global_y])\n            self.landmark_covariances[landmark_id] = sensor_uncertainty**2\n\n    def loop_closure_detection(self, current_features, threshold=0.8):\n        """\n        Detect loop closures by comparing current features with previous observations\n\n        Args:\n            current_features: Current set of observed features\n            threshold: Similarity threshold for loop closure\n\n        Returns:\n            loop_closure_candidate: Previous pose that matches current view\n        """\n        # This is a simplified approach\n        # In practice, you\'d use more sophisticated place recognition\n        if len(self.pose_graph) > 10:  # Need enough history\n            # Compare current features with features from previous poses\n            # Return index of most similar pose\n            pass\n\n        return None\n\n    def optimize_map(self):\n        """\n        Optimize map and robot trajectory using pose graph optimization\n        """\n        # This would implement graph SLAM optimization\n        # Using techniques like Gauss-Newton or Levenberg-Marquardt\n        pass\n\n# Example usage\ndef example_sensor_fusion():\n    fusion_system = MultiSensorFusion()\n\n    # Simulate sensor data over time\n    for t in np.arange(0, 5, 0.1):\n        # Simulate IMU data\n        accel = [0.1 * np.sin(t), 0.05 * np.cos(t), 9.81]\n        gyro = [0.01, 0.02, 0.03]\n        fusion_system.add_imu_data(accel, gyro, t)\n\n        # Simulate LIDAR data\n        ranges = np.random.uniform(0.5, 10.0, 360)  # 360 degree scan\n        angles = np.linspace(0, 2*np.pi, 360)\n        fusion_system.add_lidar_data(ranges, angles, t)\n\n        # Simulate camera features\n        features = [{\'x\': 100 + i*10, \'y\': 150 + i*5} for i in range(20)]\n        fusion_system.add_camera_data(features, t)\n\n        # Simulate odometry\n        pose = {\'position\': [t*0.2, t*0.1, 0.0]}\n        twist = {\'linear\': [0.2, 0.1, 0.0], \'angular\': [0.0, 0.0, 0.01]}\n        fusion_system.add_odom_data(pose, twist, t)\n\n        # Perform fusion\n        fusion_system.fuse_sensors()\n\n        if int(t*10) % 10 == 0:  # Print every second\n            print(f"Time {t:.1f}: Position={fusion_system.position}")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"urdf-examples",children:"URDF Examples"}),"\n",(0,a.jsx)(n.h3,{id:"multi-sensor-robot-platform",children:"Multi-Sensor Robot Platform"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="multi_sensor_robot">\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n      <material name="gray">\n        <color rgba="0.5 0.5 0.5 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10.0"/>\n      <inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Wheels --\x3e\n  <joint name="wheel_left_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_left"/>\n    <origin xyz="0 0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n    <dynamics damping="0.1" friction="0.0"/>\n  </joint>\n\n  <link name="wheel_left">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0.0 0.0 0.0 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.005" ixy="0.0" ixz="0.0" iyy="0.005" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  <joint name="wheel_right_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_right"/>\n    <origin xyz="0 -0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n    <dynamics damping="0.1" friction="0.0"/>\n  </joint>\n\n  <link name="wheel_right">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0.0 0.0 0.0 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.005" ixy="0.0" ixz="0.0" iyy="0.005" iyz="0.0" izz="0.01"/>\n    </inertial>\n  </link>\n\n  \x3c!-- IMU Mount --\x3e\n  <joint name="imu_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n    <origin xyz="0.0 0.0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="imu_link"/>\n\n  \x3c!-- LIDAR Mount --\x3e\n  <joint name="lidar_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0.0 0.0 0.2" rpy="0 0 0"/>\n  </joint>\n\n  <link name="lidar_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.05" length="0.05"/>\n      </geometry>\n      <material name="red">\n        <color rgba="1.0 0.0 0.0 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Camera Mount --\x3e\n  <joint name="camera_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.2 0.0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="blue">\n        <color rgba="0.0 0.0 1.0 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- GPS Mount --\x3e\n  <joint name="gps_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="gps_link"/>\n    <origin xyz="0.0 0.0 0.25" rpy="0 0 0"/>\n  </joint>\n\n  <link name="gps_link"/>\n\n  \x3c!-- Multi-Sensor Plugins --\x3e\n  <gazebo reference="imu_link">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n        <frame_name>imu_link</frame_name>\n        <topic_name>imu/data</topic_name>\n        <gaussian_noise>0.01</gaussian_noise>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="lidar_link">\n    <sensor name="lidar_sensor" type="ray">\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>360</samples>\n            <resolution>1.0</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="lidar_controller" filename="libgazebo_ros_laser.so">\n        <frame_name>lidar_link</frame_name>\n        <topic_name>scan</topic_name>\n        <gaussian_noise>0.01</gaussian_noise>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="camera_link">\n    <sensor name="camera_sensor" type="camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>camera_link</frame_name>\n        <topic_name>camera/image_raw</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="gps_link">\n    <sensor name="gps_sensor" type="gps">\n      <always_on>true</always_on>\n      <update_rate>1</update_rate>\n      <plugin name="gps_controller" filename="libgazebo_ros_gps.so">\n        <frame_name>gps_link</frame_name>\n        <topic_name>gps/fix</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Differential Drive Controller --\x3e\n  <gazebo>\n    <plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">\n      <left_joint>wheel_left_joint</left_joint>\n      <right_joint>wheel_right_joint</right_joint>\n      <wheel_separation>0.5</wheel_separation>\n      <wheel_diameter>0.2</wheel_diameter>\n      <command_topic>cmd_vel</command_topic>\n      <odometry_topic>odom</odometry_topic>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_link</robot_base_frame>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- Sensor Fusion Controller --\x3e\n  <gazebo reference="base_link">\n    <plugin name="sensor_fusion" filename="libsensor_fusion_plugin.so">\n      <imu_topic>imu/data</imu_topic>\n      <lidar_topic>scan</lidar_topic>\n      <camera_topic>camera/image_raw</camera_topic>\n      <gps_topic>gps/fix</gps_topic>\n      <odometry_topic>odom</odometry_topic>\n      <fusion_output_topic>estimated_pose</fusion_output_topic>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-fusion-pipeline-diagram",children:"Sensor Fusion Pipeline Diagram"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[IMU Data] --\x3e E[Sensor Fusion]\n    B[LIDAR Data] --\x3e E\n    C[Camera Data] --\x3e E\n    D[GPS/Odom Data] --\x3e E\n    E --\x3e F[State Estimate]\n    F --\x3e G[SLAM Map]\n    G --\x3e H[Localization]\n    H --\x3e F\n\n    style A fill:#ff9999\n    style E fill:#99ff99\n    style F fill:#99ccff\n"})}),"\n",(0,a.jsx)(n.h2,{id:"sensor-comparison",children:"Sensor Comparison"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Sensor Type"}),(0,a.jsx)(n.th,{children:"Accuracy"}),(0,a.jsx)(n.th,{children:"Update Rate"}),(0,a.jsx)(n.th,{children:"Range"}),(0,a.jsx)(n.th,{children:"Indoor/Outdoor"}),(0,a.jsx)(n.th,{children:"Key Limitations"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"IMU"}),(0,a.jsx)(n.td,{children:"Medium"}),(0,a.jsx)(n.td,{children:"100-1000Hz"}),(0,a.jsx)(n.td,{children:"Unlimited"}),(0,a.jsx)(n.td,{children:"Both"}),(0,a.jsx)(n.td,{children:"Drift over time"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"LIDAR"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"10-20Hz"}),(0,a.jsx)(n.td,{children:"10-100m"}),(0,a.jsx)(n.td,{children:"Both"}),(0,a.jsx)(n.td,{children:"Reflective surfaces"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Camera"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"30-60Hz"}),(0,a.jsx)(n.td,{children:"Depends on lighting"}),(0,a.jsx)(n.td,{children:"Both"}),(0,a.jsx)(n.td,{children:"Lighting conditions"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"GPS"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"1-10Hz"}),(0,a.jsx)(n.td,{children:"Unlimited"}),(0,a.jsx)(n.td,{children:"Outdoor"}),(0,a.jsx)(n.td,{children:"Multipath, indoors"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Encoder"}),(0,a.jsx)(n.td,{children:"High"}),(0,a.jsx)(n.td,{children:"50-200Hz"}),(0,a.jsx)(n.td,{children:"Unlimited"}),(0,a.jsx)(n.td,{children:"Both"}),(0,a.jsx)(n.td,{children:"Slip, calibration"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Ultrasonic"}),(0,a.jsx)(n.td,{children:"Low"}),(0,a.jsx)(n.td,{children:"10-50Hz"}),(0,a.jsx)(n.td,{children:"0.1-10m"}),(0,a.jsx)(n.td,{children:"Both"}),(0,a.jsx)(n.td,{children:"Surface dependency"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Fusion"}),": Combining data from multiple sensors for improved accuracy"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Kalman Filter"}),": Optimal estimator for linear systems with Gaussian noise"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Particle Filter"}),": Non-parametric filter for nonlinear/non-Gaussian systems"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SLAM"}),": Simultaneous Localization and Mapping"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Covariance"}),": Measure of uncertainty in state estimates"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Observability"}),": Ability to determine system state from outputs"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Association"}),": Matching sensor observations to known landmarks"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Loop Closure"}),": Recognizing previously visited locations"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"learning-checkpoints",children:"Learning Checkpoints"}),"\n",(0,a.jsx)(n.h3,{id:"quiz-questions",children:"Quiz Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What are the main advantages of sensor fusion over single-sensor approaches?"}),"\n",(0,a.jsx)(n.li,{children:"Explain the difference between Kalman Filter and Particle Filter."}),"\n",(0,a.jsx)(n.li,{children:"What is the observability problem in sensor fusion?"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,a.jsx)(n.p,{children:"Implement a simple Kalman Filter that fuses IMU and LIDAR data to estimate robot position."}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,a.jsx)(n.p,{children:"Create a ROS 2 node that subscribes to multiple sensor topics (IMU, LIDAR, camera) and implements a basic sensor fusion algorithm to estimate the robot's state."}),"\n",(0,a.jsx)(n.h2,{id:"personalization",children:"Personalization"}),"\n",(0,a.jsxs)("div",{className:"personalization-options",children:[(0,a.jsx)("h3",{children:"Adjust Learning Path:"}),(0,a.jsx)("button",{onClick:()=>setDifficulty("beginner"),children:"Beginner"}),(0,a.jsx)("button",{onClick:()=>setDifficulty("intermediate"),children:"Intermediate"}),(0,a.jsx)("button",{onClick:()=>setDifficulty("advanced"),children:"Advanced"})]}),"\n",(0,a.jsx)(n.h2,{id:"translation",children:"Translation"}),"\n",(0,a.jsx)("div",{className:"translation-controls",children:(0,a.jsx)("button",{onClick:()=>translateToUrdu(),children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u062a\u0631\u062c\u0645\u06c1 \u06a9\u0631\u06cc\u06ba"})})]})}function c(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var t=i(6540);const a={},s=t.createContext(a);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);