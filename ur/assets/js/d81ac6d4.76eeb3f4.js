"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[5798],{1442:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"weekly-breakdown/week6/index","title":"Week 6 - Machine Learning for Robotics","description":"Learning Objectives","source":"@site/docs/weekly-breakdown/week6/index.mdx","sourceDirName":"weekly-breakdown/week6","slug":"/weekly-breakdown/week6/","permalink":"/physical-ai-book/ur/docs/weekly-breakdown/week6/","draft":false,"unlisted":false,"editUrl":"https://github.com/fatima317/physical-ai-book/tree/main/docs/weekly-breakdown/week6/index.mdx","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Week 6 - Machine Learning for Robotics"},"sidebar":"tutorialSidebar","previous":{"title":"Week 5 - Computer Vision for Robotics","permalink":"/physical-ai-book/ur/docs/weekly-breakdown/week5/"},"next":{"title":"Week 7 - Human-Robot Interaction","permalink":"/physical-ai-book/ur/docs/weekly-breakdown/week7/"}}');var a=i(4848),r=i(8453);const s={sidebar_position:6,title:"Week 6 - Machine Learning for Robotics"},o="Week 6 - Machine Learning for Robotics",l={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Machine Learning in Robotics",id:"machine-learning-in-robotics",level:2},{value:"Code Snippets",id:"code-snippets",level:2},{value:"Supervised Learning for Robot Control",id:"supervised-learning-for-robot-control",level:3},{value:"Reinforcement Learning for Navigation",id:"reinforcement-learning-for-navigation",level:3},{value:"Deep Learning with TensorFlow/Keras",id:"deep-learning-with-tensorflowkeras",level:3},{value:"URDF Examples",id:"urdf-examples",level:2},{value:"ML-Enabled Robot with Sensors",id:"ml-enabled-robot-with-sensors",level:3},{value:"ML Pipeline Diagram",id:"ml-pipeline-diagram",level:2},{value:"Machine Learning Algorithms for Robotics",id:"machine-learning-algorithms-for-robotics",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Learning Checkpoints",id:"learning-checkpoints",level:2},{value:"Quiz Questions",id:"quiz-questions",level:3},{value:"Practical Exercise",id:"practical-exercise",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Personalization",id:"personalization",level:2},{value:"Translation",id:"translation",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"week-6---machine-learning-for-robotics",children:"Week 6 - Machine Learning for Robotics"})}),"\n",(0,a.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Apply supervised learning algorithms to robotics problems"}),"\n",(0,a.jsx)(n.li,{children:"Implement reinforcement learning for robot control"}),"\n",(0,a.jsx)(n.li,{children:"Use deep learning for perception and decision making"}),"\n",(0,a.jsx)(n.li,{children:"Train neural networks for robot behavior prediction"}),"\n",(0,a.jsx)(n.li,{children:"Integrate ML models with ROS 2 systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"machine-learning-in-robotics",children:"Machine Learning in Robotics"}),"\n",(0,a.jsx)(n.p,{children:"Machine learning is transforming robotics by enabling robots to learn from experience and adapt to new situations. Key applications include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Perception"}),": Object recognition, scene understanding"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Control"}),": Learning motor skills and navigation strategies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Prediction"}),": Anticipating human intentions and environmental changes"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Planning"}),": Learning efficient paths and manipulation strategies"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code-snippets",children:"Code Snippets"}),"\n",(0,a.jsx)(n.h3,{id:"supervised-learning-for-robot-control",children:"Supervised Learning for Robot Control"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nimport joblib\n\nclass RobotControlLearner:\n    def __init__(self):\n        self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n        self.is_trained = False\n\n    def prepare_dataset(self, sensor_data, motor_commands):\n        """\n        Prepare dataset from sensor readings and corresponding motor commands\n\n        Args:\n            sensor_data: Array of sensor readings [distance_front, distance_left, distance_right, ...]\n            motor_commands: Array of motor commands [linear_x, angular_z]\n        """\n        # Create features from sensor data\n        features = []\n        labels = []\n\n        for i in range(len(sensor_data)):\n            # Current sensor readings\n            current_sensors = sensor_data[i]\n\n            # Previous sensor readings (for temporal context)\n            prev_sensors = sensor_data[i-1] if i > 0 else np.zeros_like(sensor_data[i])\n\n            # Combined features\n            combined_features = np.concatenate([current_sensors, prev_sensors])\n\n            features.append(combined_features)\n            labels.append(motor_commands[i])\n\n        return np.array(features), np.array(labels)\n\n    def train(self, sensor_data, motor_commands):\n        """Train the robot control model"""\n        X, y = self.prepare_dataset(sensor_data, motor_commands)\n\n        # Split data into training and testing sets\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n        # Train the model\n        self.model.fit(X_train, y_train)\n\n        # Evaluate on test set\n        y_pred = self.model.predict(X_test)\n        mse = mean_squared_error(y_test, y_pred)\n\n        self.is_trained = True\n        print(f"Model trained with MSE: {mse}")\n\n        return mse\n\n    def predict_control(self, sensor_reading, prev_sensor_reading=None):\n        """Predict motor commands based on sensor readings"""\n        if not self.is_trained:\n            raise ValueError("Model must be trained first")\n\n        if prev_sensor_reading is None:\n            prev_sensor_reading = np.zeros_like(sensor_reading)\n\n        # Combine current and previous sensor readings\n        features = np.concatenate([sensor_reading, prev_sensor_reading]).reshape(1, -1)\n\n        # Predict motor commands\n        motor_cmd = self.model.predict(features)[0]\n\n        return motor_cmd\n\n    def save_model(self, filepath):\n        """Save trained model to file"""\n        joblib.dump(self.model, filepath)\n\n    def load_model(self, filepath):\n        """Load trained model from file"""\n        self.model = joblib.load(filepath)\n        self.is_trained = True\n\n# Example usage\ndef example_supervised_learning():\n    # Simulated sensor data and motor commands\n    # In practice, these would come from robot experiments\n    sensor_data = np.random.rand(1000, 5)  # 5 sensor readings\n    motor_commands = np.random.rand(1000, 2)  # linear_x, angular_z\n\n    learner = RobotControlLearner()\n    learner.train(sensor_data, motor_commands)\n\n    # Predict for new sensor reading\n    new_sensor = np.random.rand(5)\n    predicted_command = learner.predict_control(new_sensor)\n    print(f"Predicted command: {predicted_command}")\n'})}),"\n",(0,a.jsx)(n.h3,{id:"reinforcement-learning-for-navigation",children:"Reinforcement Learning for Navigation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport random\nfrom collections import defaultdict\n\nclass QLearningNavigator:\n    def __init__(self, state_space, action_space, learning_rate=0.1, discount_factor=0.95, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n        self.state_space = state_space\n        self.action_space = action_space\n        self.learning_rate = learning_rate\n        self.discount_factor = discount_factor\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.min_epsilon = min_epsilon\n\n        # Initialize Q-table\n        self.q_table = defaultdict(lambda: np.zeros(action_space))\n\n    def discretize_state(self, continuous_state):\n        """Convert continuous state to discrete state for Q-learning"""\n        # Example: discretize continuous position into grid cells\n        x, y = continuous_state\n        grid_size = 0.5  # 0.5 meter grid cells\n        discrete_x = int(x / grid_size)\n        discrete_y = int(y / grid_size)\n        return (discrete_x, discrete_y)\n\n    def choose_action(self, state):\n        """Choose action using epsilon-greedy policy"""\n        # Convert continuous state to discrete state\n        discrete_state = self.discretize_state(state)\n\n        # Exploration vs exploitation\n        if random.random() < self.epsilon:\n            # Explore: random action\n            return random.randint(0, self.action_space - 1)\n        else:\n            # Exploit: best known action\n            return np.argmax(self.q_table[discrete_state])\n\n    def update_q_value(self, state, action, reward, next_state):\n        """Update Q-value using Q-learning update rule"""\n        discrete_state = self.discretize_state(state)\n        discrete_next_state = self.discretize_state(next_state)\n\n        # Get current Q-value\n        current_q = self.q_table[discrete_state][action]\n\n        # Get max Q-value for next state\n        max_next_q = np.max(self.q_table[discrete_next_state])\n\n        # Calculate target Q-value\n        target_q = reward + self.discount_factor * max_next_q\n\n        # Update Q-value\n        self.q_table[discrete_state][action] = current_q + self.learning_rate * (target_q - current_q)\n\n    def decay_epsilon(self):\n        """Decay exploration rate"""\n        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n\nclass RobotEnvironment:\n    def __init__(self, grid_size=10):\n        self.grid_size = grid_size\n        self.robot_pos = [0, 0]  # Starting position\n        self.goal_pos = [grid_size-1, grid_size-1]  # Goal position\n        self.obstacles = [(3, 3), (4, 4), (5, 5)]  # Obstacle positions\n\n    def reset(self):\n        """Reset environment to initial state"""\n        self.robot_pos = [0, 0]\n        return self.robot_pos[:]\n\n    def step(self, action):\n        """Take action and return (next_state, reward, done)"""\n        # Define actions: 0=up, 1=right, 2=down, 3=left\n        action_effects = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n\n        # Calculate new position\n        new_row = self.robot_pos[0] + action_effects[action][0]\n        new_col = self.robot_pos[1] + action_effects[action][1]\n\n        # Check bounds\n        if 0 <= new_row < self.grid_size and 0 <= new_col < self.grid_size:\n            # Check for obstacles\n            if (new_row, new_col) not in self.obstacles:\n                self.robot_pos = [new_row, new_col]\n\n        # Calculate reward\n        reward = -0.1  # Small negative reward for each step (encourage efficiency)\n\n        # Check if reached goal\n        if self.robot_pos[0] == self.goal_pos[0] and self.robot_pos[1] == self.goal_pos[1]:\n            reward = 10  # Large positive reward for reaching goal\n            done = True\n        elif (self.robot_pos[0], self.robot_pos[1]) in self.obstacles:\n            reward = -5  # Penalty for hitting obstacle\n            done = False\n        else:\n            # Distance-based reward (closer to goal = better)\n            dist_to_goal = abs(self.robot_pos[0] - self.goal_pos[0]) + abs(self.robot_pos[1] - self.goal_pos[1])\n            reward -= dist_to_goal * 0.01\n            done = False\n\n        return self.robot_pos[:], reward, done\n\n# Example training loop\ndef train_navigation_agent(episodes=1000):\n    env = RobotEnvironment()\n    agent = QLearningNavigator(state_space=100, action_space=4)  # 10x10 grid -> 100 states, 4 actions\n\n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n\n        while True:\n            action = agent.choose_action(state)\n            next_state, reward, done = env.step(action)\n\n            agent.update_q_value(state, action, reward, next_state)\n            state = next_state\n            total_reward += reward\n\n            if done:\n                break\n\n        agent.decay_epsilon()\n\n        if episode % 100 == 0:\n            print(f"Episode {episode}, Total Reward: {total_reward}, Epsilon: {agent.epsilon}")\n\n    return agent\n'})}),"\n",(0,a.jsx)(n.h3,{id:"deep-learning-with-tensorflowkeras",children:"Deep Learning with TensorFlow/Keras"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\n\nclass RobotDeepLearner:\n    def __init__(self, input_shape, num_actions):\n        self.input_shape = input_shape\n        self.num_actions = num_actions\n        self.model = self.build_model()\n\n    def build_model(self):\n        """Build neural network for robot learning"""\n        model = keras.Sequential([\n            keras.layers.Dense(128, activation=\'relu\', input_shape=self.input_shape),\n            keras.layers.Dropout(0.3),\n            keras.layers.Dense(64, activation=\'relu\'),\n            keras.layers.Dropout(0.3),\n            keras.layers.Dense(32, activation=\'relu\'),\n            keras.layers.Dense(self.num_actions, activation=\'linear\')  # No activation for regression\n        ])\n\n        model.compile(\n            optimizer=\'adam\',\n            loss=\'mse\',\n            metrics=[\'mae\']\n        )\n\n        return model\n\n    def preprocess_state(self, state):\n        """Preprocess state for neural network"""\n        # Normalize state values\n        state = np.array(state)\n        state = state.astype(np.float32)\n\n        # Add batch dimension if needed\n        if len(state.shape) == 1:\n            state = state.reshape(1, -1)\n\n        return state\n\n    def predict_action_values(self, state):\n        """Predict Q-values for all actions"""\n        state = self.preprocess_state(state)\n        q_values = self.model.predict(state, verbose=0)\n        return q_values[0]  # Remove batch dimension\n\n    def train_batch(self, states, targets, epochs=1):\n        """Train on a batch of experiences"""\n        states = self.preprocess_state(states)\n        targets = np.array(targets)\n\n        history = self.model.fit(\n            states, targets,\n            epochs=epochs,\n            verbose=0,\n            shuffle=True\n        )\n\n        return history.history[\'loss\'][-1]  # Return final loss\n\n    def save_model(self, filepath):\n        """Save model to file"""\n        self.model.save(filepath)\n\n    def load_model(self, filepath):\n        """Load model from file"""\n        self.model = keras.models.load_model(filepath)\n\n# Deep Q-Network (DQN) Agent\nclass DQNAgent:\n    def __init__(self, input_shape, num_actions, learning_rate=0.001, gamma=0.95, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n        self.input_shape = input_shape\n        self.num_actions = num_actions\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.min_epsilon = min_epsilon\n\n        # Main network\n        self.main_network = RobotDeepLearner(input_shape, num_actions)\n\n        # Target network (for stable training)\n        self.target_network = RobotDeepLearner(input_shape, num_actions)\n\n        # Copy weights from main to target network\n        self.update_target_network()\n\n    def update_target_network(self):\n        """Copy weights from main network to target network"""\n        self.target_network.model.set_weights(self.main_network.model.get_weights())\n\n    def choose_action(self, state, training=True):\n        """Choose action using epsilon-greedy policy"""\n        if training and np.random.random() < self.epsilon:\n            # Explore: random action\n            return np.random.choice(self.num_actions)\n        else:\n            # Exploit: best known action\n            q_values = self.main_network.predict_action_values(state)\n            return np.argmax(q_values)\n\n    def train(self, experiences, batch_size=32):\n        """Train on batch of experiences"""\n        if len(experiences) < batch_size:\n            return None\n\n        # Sample random batch\n        batch_indices = np.random.choice(len(experiences), size=batch_size, replace=False)\n        batch = [experiences[i] for i in batch_indices]\n\n        # Extract components\n        states = np.array([exp[0] for exp in batch])\n        actions = np.array([exp[1] for exp in batch])\n        rewards = np.array([exp[2] for exp in batch])\n        next_states = np.array([exp[3] for exp in batch])\n        dones = np.array([exp[4] for exp in batch])\n\n        # Predict Q-values for current states\n        current_q_values = self.main_network.model.predict(states, verbose=0)\n\n        # Predict Q-values for next states using target network\n        next_q_values = self.target_network.model.predict(next_states, verbose=0)\n\n        # Calculate target Q-values\n        targets = current_q_values.copy()\n        for i in range(batch_size):\n            if dones[i]:\n                targets[i][actions[i]] = rewards[i]\n            else:\n                targets[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n\n        # Train main network\n        loss = self.main_network.train_batch(states, targets)\n\n        # Decay epsilon\n        if self.epsilon > self.min_epsilon:\n            self.epsilon *= self.epsilon_decay\n\n        return loss\n\n    def save_model(self, filepath):\n        """Save both main and target networks"""\n        self.main_network.save_model(filepath + "_main.h5")\n        self.target_network.save_model(filepath + "_target.h5")\n\n    def load_model(self, filepath):\n        """Load both main and target networks"""\n        self.main_network.load_model(filepath + "_main.h5")\n        self.target_network.load_model(filepath + "_target.h5")\n'})}),"\n",(0,a.jsx)(n.h2,{id:"urdf-examples",children:"URDF Examples"}),"\n",(0,a.jsx)(n.h3,{id:"ml-enabled-robot-with-sensors",children:"ML-Enabled Robot with Sensors"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="ml_robot">\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n      <material name="dark_gray">\n        <color rgba="0.3 0.3 0.3 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10.0"/>\n      <inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Wheels --\x3e\n  <joint name="wheel_left_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_left"/>\n    <origin xyz="0 0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="wheel_left">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  <joint name="wheel_right_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_right"/>\n    <origin xyz="0 -0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="wheel_right">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- LIDAR Mount --\x3e\n  <joint name="lidar_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_link"/>\n    <origin xyz="0.0 0.0 0.2" rpy="0 0 0"/>\n  </joint>\n\n  <link name="lidar_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.05" length="0.05"/>\n      </geometry>\n    </visual>\n  </link>\n\n  \x3c!-- Camera Mount --\x3e\n  <joint name="camera_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_link"/>\n    <origin xyz="0.2 0.0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="camera_link">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </visual>\n  </link>\n\n  \x3c!-- IMU Mount --\x3e\n  <joint name="imu_mount_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_link"/>\n    <origin xyz="0.0 0.0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  <link name="imu_link"/>\n\n  \x3c!-- ML Sensors --\x3e\n  <gazebo reference="lidar_link">\n    <sensor name="lidar" type="ray">\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>360</samples>\n            <resolution>1.0</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>10.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n        <frame_name>lidar_link</frame_name>\n        <topic_name>scan</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="camera_link">\n    <sensor name="camera" type="camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n      <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>camera_link</frame_name>\n        <topic_name>camera/image_raw</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="imu_link">\n    <sensor name="imu" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n        <frame_name>imu_link</frame_name>\n        <topic_name>imu</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Differential Drive Controller --\x3e\n  <gazebo>\n    <plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">\n      <left_joint>wheel_left_joint</left_joint>\n      <right_joint>wheel_right_joint</right_joint>\n      <wheel_separation>0.5</wheel_separation>\n      <wheel_diameter>0.2</wheel_diameter>\n      <command_topic>cmd_vel</command_topic>\n      <odometry_topic>odom</odometry_topic>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_link</robot_base_frame>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- ML Controller Plugin --\x3e\n  <gazebo reference="base_link">\n    <plugin name="ml_controller" filename="libml_controller.so">\n      <lidar_topic>scan</lidar_topic>\n      <camera_topic>camera/image_raw</camera_topic>\n      <imu_topic>imu</imu_topic>\n      <command_topic>cmd_vel</command_topic>\n      <model_file>/models/ml_robot_model.h5</model_file>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,a.jsx)(n.h2,{id:"ml-pipeline-diagram",children:"ML Pipeline Diagram"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Sensor Data] --\x3e B[Data Preprocessing]\n    B --\x3e C[Feature Engineering]\n    C --\x3e D[Model Training]\n    D --\x3e E[Model Evaluation]\n    E --\x3e F[Deployment]\n    F --\x3e G[Robot Action]\n    G --\x3e H[Environment Feedback]\n    H --\x3e A\n\n    style A fill:#ff9999\n    style D fill:#99ff99\n    style G fill:#99ccff\n"})}),"\n",(0,a.jsx)(n.h2,{id:"machine-learning-algorithms-for-robotics",children:"Machine Learning Algorithms for Robotics"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Algorithm"}),(0,a.jsx)(n.th,{children:"Type"}),(0,a.jsx)(n.th,{children:"Application"}),(0,a.jsx)(n.th,{children:"Advantages"}),(0,a.jsx)(n.th,{children:"Disadvantages"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Random Forest"}),(0,a.jsx)(n.td,{children:"Supervised"}),(0,a.jsx)(n.td,{children:"Control prediction"}),(0,a.jsx)(n.td,{children:"Robust, handles outliers"}),(0,a.jsx)(n.td,{children:"Black box, memory intensive"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"SVM"}),(0,a.jsx)(n.td,{children:"Supervised"}),(0,a.jsx)(n.td,{children:"Classification"}),(0,a.jsx)(n.td,{children:"Effective in high dimensions"}),(0,a.jsx)(n.td,{children:"Sensitive to parameters"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Q-Learning"}),(0,a.jsx)(n.td,{children:"Reinforcement"}),(0,a.jsx)(n.td,{children:"Navigation"}),(0,a.jsx)(n.td,{children:"Theoretical guarantees"}),(0,a.jsx)(n.td,{children:"Slow convergence"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Deep Q-Network"}),(0,a.jsx)(n.td,{children:"Deep RL"}),(0,a.jsx)(n.td,{children:"Complex control"}),(0,a.jsx)(n.td,{children:"Handles high-dim inputs"}),(0,a.jsx)(n.td,{children:"Sample inefficient"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"LSTM"}),(0,a.jsx)(n.td,{children:"Sequential"}),(0,a.jsx)(n.td,{children:"Trajectory prediction"}),(0,a.jsx)(n.td,{children:"Handles sequences"}),(0,a.jsx)(n.td,{children:"Computationally expensive"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"CNN"}),(0,a.jsx)(n.td,{children:"Deep Learning"}),(0,a.jsx)(n.td,{children:"Perception"}),(0,a.jsx)(n.td,{children:"Spatial feature extraction"}),(0,a.jsx)(n.td,{children:"Requires lots of data"})]})]})]}),"\n",(0,a.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Supervised Learning"}),": Learning from labeled examples"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reinforcement Learning"}),": Learning through trial and error with rewards"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Deep Learning"}),": Neural networks with multiple layers"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Q-Learning"}),": Value-based reinforcement learning algorithm"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Experience Replay"}),": Storing and sampling past experiences"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Target Network"}),": Stable network for training deep RL agents"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Epsilon-Greedy"}),": Exploration-exploitation strategy"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Feature Engineering"}),": Creating meaningful input representations"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"learning-checkpoints",children:"Learning Checkpoints"}),"\n",(0,a.jsx)(n.h3,{id:"quiz-questions",children:"Quiz Questions"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"What is the main advantage of using deep learning over traditional ML for robotics perception?"}),"\n",(0,a.jsx)(n.li,{children:"Explain the purpose of the target network in DQN."}),"\n",(0,a.jsx)(n.li,{children:"What is experience replay and why is it important in deep RL?"}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,a.jsx)(n.p,{children:"Implement a simple supervised learning model that predicts robot wheel velocities based on LIDAR scan data for obstacle avoidance."}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,a.jsx)(n.p,{children:"Create a ROS 2 node that implements a basic reinforcement learning agent for navigation. The agent should learn to navigate to a goal while avoiding obstacles based on sensor feedback."}),"\n",(0,a.jsx)(n.h2,{id:"personalization",children:"Personalization"}),"\n",(0,a.jsxs)("div",{className:"personalization-options",children:[(0,a.jsx)("h3",{children:"Adjust Learning Path:"}),(0,a.jsx)("button",{onClick:()=>setDifficulty("beginner"),children:"Beginner"}),(0,a.jsx)("button",{onClick:()=>setDifficulty("intermediate"),children:"Intermediate"}),(0,a.jsx)("button",{onClick:()=>setDifficulty("advanced"),children:"Advanced"})]}),"\n",(0,a.jsx)(n.h2,{id:"translation",children:"Translation"}),"\n",(0,a.jsx)("div",{className:"translation-controls",children:(0,a.jsx)("button",{onClick:()=>translateToUrdu(),children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u062a\u0631\u062c\u0645\u06c1 \u06a9\u0631\u06cc\u06ba"})})]})}function p(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var t=i(6540);const a={},r=t.createContext(a);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(r.Provider,{value:n},e.children)}}}]);