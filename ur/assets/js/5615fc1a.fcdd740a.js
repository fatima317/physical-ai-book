"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[9891],{8453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>a});var t=i(6540);const o={},r=t.createContext(o);function s(e){const n=t.useContext(r);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(r.Provider,{value:n},e.children)}},9790:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"weekly-breakdown/week7/index","title":"Week 7 - Human-Robot Interaction","description":"Learning Objectives","source":"@site/docs/weekly-breakdown/week7/index.mdx","sourceDirName":"weekly-breakdown/week7","slug":"/weekly-breakdown/week7/","permalink":"/physical-ai-book/ur/docs/weekly-breakdown/week7/","draft":false,"unlisted":false,"editUrl":"https://github.com/fatima317/physical-ai-book/tree/main/docs/weekly-breakdown/week7/index.mdx","tags":[],"version":"current","sidebarPosition":7,"frontMatter":{"sidebar_position":7,"title":"Week 7 - Human-Robot Interaction"},"sidebar":"tutorialSidebar","previous":{"title":"Week 6 - Machine Learning for Robotics","permalink":"/physical-ai-book/ur/docs/weekly-breakdown/week6/"},"next":{"title":"Week 8 - Manipulation and Grasping","permalink":"/physical-ai-book/ur/docs/weekly-breakdown/week8/"}}');var o=i(4848),r=i(8453);const s={sidebar_position:7,title:"Week 7 - Human-Robot Interaction"},a="Week 7 - Human-Robot Interaction",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Human-Robot Interaction Principles",id:"human-robot-interaction-principles",level:2},{value:"Code Snippets",id:"code-snippets",level:2},{value:"Voice Command Processing",id:"voice-command-processing",level:3},{value:"Gesture Recognition System",id:"gesture-recognition-system",level:3},{value:"Social Behavior System",id:"social-behavior-system",level:3},{value:"URDF Examples",id:"urdf-examples",level:2},{value:"Social Robot with Expression Capabilities",id:"social-robot-with-expression-capabilities",level:3},{value:"HRI Pipeline Diagram",id:"hri-pipeline-diagram",level:2},{value:"HRI Modalities Comparison",id:"hri-modalities-comparison",level:2},{value:"Key Terms",id:"key-terms",level:2},{value:"Learning Checkpoints",id:"learning-checkpoints",level:2},{value:"Quiz Questions",id:"quiz-questions",level:3},{value:"Practical Exercise",id:"practical-exercise",level:3},{value:"Hands-On Exercise",id:"hands-on-exercise",level:2},{value:"Personalization",id:"personalization",level:2},{value:"Translation",id:"translation",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"week-7---human-robot-interaction",children:"Week 7 - Human-Robot Interaction"})}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this week, you will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Design intuitive interfaces for human-robot interaction"}),"\n",(0,o.jsx)(n.li,{children:"Implement voice command processing with speech recognition"}),"\n",(0,o.jsx)(n.li,{children:"Create gesture recognition systems for robot control"}),"\n",(0,o.jsx)(n.li,{children:"Develop social robotics behaviors and expressions"}),"\n",(0,o.jsx)(n.li,{children:"Integrate multimodal interaction systems (voice, gesture, touch)"}),"\n",(0,o.jsx)(n.li,{children:"Implement safety protocols for human-robot interaction"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"human-robot-interaction-principles",children:"Human-Robot Interaction Principles"}),"\n",(0,o.jsx)(n.p,{children:"Human-Robot Interaction (HRI) focuses on designing robots that can effectively communicate and collaborate with humans. Key principles include:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Transparency"}),": Robots should communicate their intentions clearly"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Predictability"}),": Robot behavior should be understandable and consistent"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Trust"}),": Building confidence in robot capabilities and safety"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Cues"}),": Using human-like communication signals"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Adaptability"}),": Adjusting to individual human preferences and abilities"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"code-snippets",children:"Code Snippets"}),"\n",(0,o.jsx)(n.h3,{id:"voice-command-processing",children:"Voice Command Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport speech_recognition as sr\nimport pyttsx3\nimport threading\nimport queue\n\nclass VoiceCommandNode(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_node\')\n\n        # Initialize speech recognizer and synthesizer\n        self.recognizer = sr.Recognizer()\n        self.microphone = sr.Microphone()\n\n        # Initialize text-to-speech engine\n        self.tts_engine = pyttsx3.init()\n        self.tts_engine.setProperty(\'rate\', 150)  # Speed of speech\n        self.tts_engine.setProperty(\'volume\', 0.9)  # Volume level\n\n        # Adjust microphone for ambient noise\n        with self.microphone as source:\n            self.recognizer.adjust_for_ambient_noise(source)\n\n        # Publishers and subscribers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.voice_cmd_pub = self.create_publisher(String, \'voice_commands\', 10)\n\n        # Command queue for processing\n        self.command_queue = queue.Queue()\n\n        # Start voice recognition thread\n        self.voice_thread = threading.Thread(target=self.listen_for_voice_commands)\n        self.voice_thread.daemon = True\n        self.voice_thread.start()\n\n        # Timer to process commands\n        self.command_timer = self.create_timer(0.1, self.process_commands)\n\n    def listen_for_voice_commands(self):\n        """Continuously listen for voice commands"""\n        with self.microphone as source:\n            while True:\n                try:\n                    # Listen for audio with timeout\n                    self.get_logger().info("Listening for voice commands...")\n                    audio = self.recognizer.listen(source, timeout=1, phrase_time_limit=5)\n\n                    # Recognize speech using Google\'s service\n                    command_text = self.recognizer.recognize_google(audio)\n\n                    self.get_logger().info(f"Heard command: {command_text}")\n\n                    # Add to command queue\n                    self.command_queue.put(command_text.lower())\n\n                except sr.WaitTimeoutError:\n                    # Continue listening if no audio detected\n                    continue\n                except sr.UnknownValueError:\n                    self.get_logger().warn("Could not understand audio")\n                    continue\n                except sr.RequestError as e:\n                    self.get_logger().error(f"Could not request results; {e}")\n                    continue\n                except Exception as e:\n                    self.get_logger().error(f"Error in voice recognition: {e}")\n                    continue\n\n    def process_commands(self):\n        """Process voice commands from queue"""\n        try:\n            while True:  # Process all available commands\n                command = self.command_queue.get_nowait()\n\n                # Process the command\n                self.handle_voice_command(command)\n\n        except queue.Empty:\n            # No commands to process\n            pass\n\n    def handle_voice_command(self, command):\n        """Handle recognized voice commands"""\n        cmd_vel = Twist()\n\n        if \'forward\' in command or \'go straight\' in command or \'move ahead\' in command:\n            cmd_vel.linear.x = 0.5  # Move forward\n            self.speak_response("Moving forward")\n        elif \'backward\' in command or \'back\' in command:\n            cmd_vel.linear.x = -0.5  # Move backward\n            self.speak_response("Moving backward")\n        elif \'turn left\' in command or \'left\' in command:\n            cmd_vel.angular.z = 0.5  # Turn left\n            self.speak_response("Turning left")\n        elif \'turn right\' in command or \'right\' in command:\n            cmd_vel.angular.z = -0.5  # Turn right\n            self.speak_response("Turning right")\n        elif \'stop\' in command or \'halt\' in command:\n            # Stop movement (Twist is already zero)\n            self.speak_response("Stopping")\n        elif \'hello\' in command or \'hi\' in command:\n            self.speak_response("Hello! How can I help you?")\n        elif \'help\' in command:\n            self.speak_response("I can move forward, backward, turn left, turn right, or stop. Say \'help\' for more options.")\n        else:\n            self.speak_response(f"I don\'t understand \'{command}\'. Say \'help\' for available commands.")\n            return\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n        # Publish voice command for logging\n        cmd_msg = String()\n        cmd_msg.data = command\n        self.voice_cmd_pub.publish(cmd_msg)\n\n    def speak_response(self, text):\n        """Speak response using text-to-speech"""\n        self.get_logger().info(f"Speaking: {text}")\n        self.tts_engine.say(text)\n        self.tts_engine.runAndWait()\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    voice_node = VoiceCommandNode()\n\n    try:\n        rclpy.spin(voice_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        voice_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"gesture-recognition-system",children:"Gesture Recognition System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import cv2\nimport mediapipe as mp\nimport numpy as np\nfrom collections import deque\nimport math\n\nclass GestureRecognizer:\n    def __init__(self):\n        # Initialize MediaPipe Hands\n        self.mp_hands = mp.solutions.hands\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.7,\n            min_tracking_confidence=0.5\n        )\n        self.mp_draw = mp.solutions.drawing_utils\n\n        # Gesture history for smoothing\n        self.gesture_history = deque(maxlen=10)\n\n        # Finger tip indices\n        self.tip_ids = [4, 8, 12, 16, 20]  # Thumb, Index, Middle, Ring, Pinky\n\n    def process_frame(self, frame):\n        \"\"\"Process frame for hand gestures\"\"\"\n        # Convert BGR to RGB\n        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n\n        # Process the frame\n        results = self.hands.process(rgb_frame)\n\n        gesture_result = {\n            'hands_detected': 0,\n            'gestures': [],\n            'hand_positions': []\n        }\n\n        if results.multi_hand_landmarks:\n            gesture_result['hands_detected'] = len(results.multi_hand_landmarks)\n\n            for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n                # Draw landmarks\n                self.mp_draw.draw_landmarks(\n                    frame,\n                    hand_landmarks,\n                    self.mp_hands.HAND_CONNECTIONS\n                )\n\n                # Analyze gesture\n                gesture = self.analyze_gesture(hand_landmarks, frame)\n                gesture_result['gestures'].append(gesture)\n\n                # Get hand position\n                hand_pos = self.get_hand_position(hand_landmarks, frame)\n                gesture_result['hand_positions'].append(hand_pos)\n\n                # Add gesture to history\n                self.gesture_history.append(gesture['gesture_type'])\n\n        return frame, gesture_result\n\n    def analyze_gesture(self, landmarks, frame):\n        \"\"\"Analyze finger positions to determine gesture\"\"\"\n        # Get landmark positions\n        landmark_list = []\n        for id, lm in enumerate(landmarks.landmark):\n            h, w, c = frame.shape\n            cx, cy = int(lm.x * w), int(lm.y * h)\n            landmark_list.append([id, cx, cy])\n\n        if len(landmark_list) != 0:\n            # Determine which fingers are up\n            fingers = []\n\n            # Thumb: compare thumb tip with thumb ip joint\n            if len(landmark_list) > 4:\n                if landmark_list[self.tip_ids[0]][1] < landmark_list[self.tip_ids[0] - 2][1]:  # Right hand\n                    fingers.append(1)\n                elif landmark_list[self.tip_ids[0]][1] > landmark_list[self.tip_ids[0] - 2][1]:  # Left hand\n                    fingers.append(1)\n                else:\n                    fingers.append(0)\n\n            # Other fingers: compare tip with pip joint\n            for id in range(1, 5):\n                if len(landmark_list) > self.tip_ids[id]:\n                    if landmark_list[self.tip_ids[id]][2] < landmark_list[self.tip_ids[id] - 2][2]:\n                        fingers.append(1)\n                    else:\n                        fingers.append(0)\n\n            # Count raised fingers\n            total_fingers = sum(fingers)\n\n            # Determine gesture based on finger configuration\n            gesture_type = self.classify_gesture(fingers, landmark_list)\n\n            return {\n                'gesture_type': gesture_type,\n                'finger_count': total_fingers,\n                'fingers_up': fingers\n            }\n\n        return {'gesture_type': 'unknown', 'finger_count': 0, 'fingers_up': []}\n\n    def classify_gesture(self, fingers, landmarks):\n        \"\"\"Classify gesture based on finger configuration\"\"\"\n        # Thumbs up\n        if fingers == [1, 0, 0, 0, 0]:\n            return 'thumbs_up'\n        # Peace sign (index and middle up)\n        elif fingers == [0, 1, 1, 0, 0]:\n            return 'peace'\n        # Victory (index up)\n        elif fingers == [0, 1, 0, 0, 0]:\n            return 'victory'\n        # Rock and roll (index and pinky up)\n        elif fingers == [0, 1, 0, 0, 1]:\n            return 'rock'\n        # Stop (all fingers up)\n        elif fingers == [1, 1, 1, 1, 1]:\n            return 'stop'\n        # Fist (all fingers down)\n        elif fingers == [0, 0, 0, 0, 0]:\n            return 'fist'\n        # Pointing (index finger up, others down)\n        elif fingers == [0, 1, 0, 0, 0]:\n            return 'point'\n        else:\n            return 'other'\n\n    def get_hand_position(self, landmarks, frame):\n        \"\"\"Get the center position of the hand\"\"\"\n        landmark_list = []\n        for lm in landmarks.landmark:\n            h, w, c = frame.shape\n            cx, cy = int(lm.x * w), int(lm.y * h)\n            landmark_list.append([cx, cy])\n\n        if landmark_list:\n            # Calculate centroid of all landmarks\n            avg_x = sum([pt[0] for pt in landmark_list]) // len(landmark_list)\n            avg_y = sum([pt[1] for pt in landmark_list]) // len(landmark_list)\n            return (avg_x, avg_y)\n\n        return (0, 0)\n\n    def get_smoothed_gesture(self):\n        \"\"\"Get smoothed gesture from history\"\"\"\n        if len(self.gesture_history) == 0:\n            return 'unknown'\n\n        # Get most common gesture in history\n        gesture_counts = {}\n        for gesture in self.gesture_history:\n            gesture_counts[gesture] = gesture_counts.get(gesture, 0) + 1\n\n        return max(gesture_counts, key=gesture_counts.get)\n\nclass GestureControlNode:\n    def __init__(self):\n        import rclpy\n        from rclpy.node import Node\n        from sensor_msgs.msg import Image\n        from geometry_msgs.msg import Twist\n        from cv_bridge import CvBridge\n\n        self.node = Node('gesture_control_node')\n        self.bridge = CvBridge()\n\n        # Publishers\n        self.cmd_vel_pub = self.node.create_publisher(Twist, 'cmd_vel', 10)\n        self.image_pub = self.node.create_publisher(Image, 'gesture_output', 10)\n\n        # Subscribers\n        self.image_sub = self.node.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.image_callback,\n            10\n        )\n\n        # Gesture recognizer\n        self.gestureRecognizer = GestureRecognizer()\n\n        # Last processed gesture\n        self.last_gesture = 'unknown'\n\n        self.node.get_logger().info(\"Gesture control node initialized\")\n\n    def image_callback(self, msg):\n        \"\"\"Process incoming image for gesture recognition\"\"\"\n        try:\n            # Convert ROS image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \"bgr8\")\n\n            # Process frame for gestures\n            processed_frame, gesture_result = self.gestureRecognizer.process_frame(cv_image)\n\n            # Handle gestures\n            if gesture_result['gestures']:\n                for gesture in gesture_result['gestures']:\n                    if gesture['gesture_type'] != self.last_gesture:\n                        self.handle_gesture(gesture['gesture_type'])\n                        self.last_gesture = gesture['gesture_type']\n\n            # Convert back to ROS image and publish\n            output_msg = self.bridge.cv2_to_imgmsg(processed_frame, \"bgr8\")\n            output_msg.header = msg.header\n            self.image_pub.publish(output_msg)\n\n        except Exception as e:\n            self.node.get_logger().error(f\"Error processing image: {e}\")\n\n    def handle_gesture(self, gesture_type):\n        \"\"\"Handle recognized gesture\"\"\"\n        from geometry_msgs.msg import Twist\n\n        cmd_vel = Twist()\n\n        if gesture_type == 'thumbs_up':\n            cmd_vel.linear.x = 0.3  # Move forward slowly\n            self.node.get_logger().info(\"Gesture: Thumbs up - Moving forward\")\n        elif gesture_type == 'peace':\n            cmd_vel.angular.z = 0.5  # Turn left\n            self.node.get_logger().info(\"Gesture: Peace - Turning left\")\n        elif gesture_type == 'rock':\n            cmd_vel.angular.z = -0.5  # Turn right\n            self.node.get_logger().info(\"Gesture: Rock - Turning right\")\n        elif gesture_type == 'stop':\n            # Stop (Twist is already zero)\n            self.node.get_logger().info(\"Gesture: Stop - Stopping\")\n        elif gesture_type == 'fist':\n            cmd_vel.linear.x = -0.3  # Move backward slowly\n            self.node.get_logger().info(\"Gesture: Fist - Moving backward\")\n        else:\n            self.node.get_logger().info(f\"Recognized gesture: {gesture_type}\")\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\ndef main_gesture():\n    import rclpy\n    rclpy.init()\n\n    gesture_node = GestureControlNode()\n\n    try:\n        rclpy.spin(gesture_node.node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        gesture_node.node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,o.jsx)(n.h3,{id:"social-behavior-system",children:"Social Behavior System"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import time\nimport json\nfrom enum import Enum\n\nclass RobotEmotionalState(Enum):\n    NEUTRAL = "neutral"\n    HAPPY = "happy"\n    EXCITED = "excited"\n    CONFUSED = "confused"\n    ATTENTIVE = "attentive"\n    SLEEPY = "sleepy"\n\nclass SocialBehaviorManager:\n    def __init__(self, robot_name="Robot"):\n        self.robot_name = robot_name\n        self.emotional_state = RobotEmotionalState.NEUTRAL\n        self.interaction_history = []\n        self.last_interaction_time = time.time()\n        self.social_rules = self.load_social_rules()\n\n    def load_social_rules(self):\n        """Load social interaction rules"""\n        return {\n            "greeting_responses": {\n                "hello": ["Hello!", "Hi there!", "Greetings!"],\n                "hi": ["Hi!", "Hello!", "Hey!"],\n                "good morning": ["Good morning!", "Morning!"],\n                "good evening": ["Good evening!", "Evening!"]\n            },\n            "farewell_responses": {\n                "bye": ["Goodbye!", "See you later!", "Bye!"],\n                "goodbye": ["Goodbye!", "Take care!", "See you!"],\n                "see you": ["See you!", "Later!", "Bye!"]\n            },\n            "positive_interactions": ["thank you", "nice", "great", "good job", "awesome"],\n            "negative_interactions": ["sorry", "wrong", "stop", "not working"]\n        }\n\n    def update_emotional_state(self, interaction_type="neutral"):\n        """Update robot\'s emotional state based on interaction"""\n        current_time = time.time()\n\n        if interaction_type == "positive":\n            if self.emotional_state in [RobotEmotionalState.NEUTRAL, RobotEmotionalState.ATTENTIVE]:\n                self.emotional_state = RobotEmotionalState.HAPPY\n            elif self.emotional_state == RobotEmotionalState.HAPPY:\n                self.emotional_state = RobotEmotionalState.EXCITED\n        elif interaction_type == "negative":\n            if self.emotional_state in [RobotEmotionalState.HAPPY, RobotEmotionalState.EXCITED]:\n                self.emotional_state = RobotEmotionalState.CONFUSED\n        elif interaction_type == "attention":\n            self.emotional_state = RobotEmotionalState.ATTENTIVE\n        elif current_time - self.last_interaction_time > 30:  # 30 seconds of inactivity\n            if self.emotional_state != RobotEmotionalState.SLEEPY:\n                self.emotional_state = RobotEmotionalState.SLEEPY\n\n        self.last_interaction_time = current_time\n\n    def generate_response(self, user_input):\n        """Generate appropriate social response"""\n        user_input_lower = user_input.lower()\n\n        # Check for greetings\n        for greeting, responses in self.social_rules["greeting_responses"].items():\n            if greeting in user_input_lower:\n                self.update_emotional_state("positive")\n                return responses[0]  # Could be randomized\n\n        # Check for farewells\n        for farewell, responses in self.social_rules["farewell_responses"].items():\n            if farewell in user_input_lower:\n                self.update_emotional_state("neutral")\n                return responses[0]\n\n        # Check for positive interactions\n        for pos_phrase in self.social_rules["positive_interactions"]:\n            if pos_phrase in user_input_lower:\n                self.update_emotional_state("positive")\n                return f"Thank you! I\'m happy to help!"\n\n        # Check for negative interactions\n        for neg_phrase in self.social_rules["negative_interactions"]:\n            if neg_phrase in user_input_lower:\n                self.update_emotional_state("negative")\n                return f"I apologize. How can I assist you better?"\n\n        # Default response\n        self.update_emotional_state("attention")\n        return f"Hello! I\'m {self.robot_name}. How can I assist you today?"\n\n    def get_behavior_indicators(self):\n        """Get behavioral indicators based on emotional state"""\n        behaviors = {\n            RobotEmotionalState.NEUTRAL: {\n                "speed": "normal",\n                "tone": "friendly",\n                "movements": "moderate"\n            },\n            RobotEmotionalState.HAPPY: {\n                "speed": "lively",\n                "tone": "cheerful",\n                "movements": "animated"\n            },\n            RobotEmotionalState.EXCITED: {\n                "speed": "fast",\n                "tone": "enthusiastic",\n                "movements": "expressive"\n            },\n            RobotEmotionalState.CONFUSED: {\n                "speed": "slow",\n                "tone": "cautious",\n                "movements": "hesitant"\n            },\n            RobotEmotionalState.ATTENTIVE: {\n                "speed": "responsive",\n                "tone": "focused",\n                "movements": "attentive"\n            },\n            RobotEmotionalState.SLEEPY: {\n                "speed": "slow",\n                "tone": "calm",\n                "movements": "minimal"\n            }\n        }\n\n        return behaviors.get(self.emotional_state, behaviors[RobotEmotionalState.NEUTRAL])\n\n# Example usage\ndef example_social_behavior():\n    sbm = SocialBehaviorManager("Robbie")\n\n    # Simulate interactions\n    interactions = [\n        "hello",\n        "thank you",\n        "that\'s great!",\n        "something went wrong",\n        "bye"\n    ]\n\n    for interaction in interactions:\n        response = sbm.generate_response(interaction)\n        behavior = sbm.get_behavior_indicators()\n\n        print(f"User: {interaction}")\n        print(f"Robot: {response}")\n        print(f"Behavior: {behavior}")\n        print("---")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"urdf-examples",children:"URDF Examples"}),"\n",(0,o.jsx)(n.h3,{id:"social-robot-with-expression-capabilities",children:"Social Robot with Expression Capabilities"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0"?>\n<robot name="social_robot">\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n      <material name="light_blue">\n        <color rgba="0.5 0.7 1.0 1.0"/>\n      </material>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder radius="0.3" length="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="10.0"/>\n      <inertia ixx="0.4" ixy="0.0" ixz="0.0" iyy="0.4" iyz="0.0" izz="0.2"/>\n    </inertial>\n  </link>\n\n  \x3c!-- Wheels --\x3e\n  <joint name="wheel_left_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_left"/>\n    <origin xyz="0 0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="wheel_left">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  <joint name="wheel_right_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="wheel_right"/>\n    <origin xyz="0 -0.25 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="wheel_right">\n    <visual>\n      <geometry>\n        <cylinder radius="0.1" length="0.05"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Torso --\x3e\n  <joint name="torso_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="torso"/>\n    <origin xyz="0.0 0.0 0.075" rpy="0 0 0"/>\n  </joint>\n\n  <link name="torso">\n    <visual>\n      <geometry>\n        <cylinder radius="0.15" length="0.3"/>\n      </geometry>\n      <material name="white">\n        <color rgba="1.0 1.0 1.0 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Head --\x3e\n  <joint name="neck_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="head"/>\n    <origin xyz="0.0 0.0 0.3" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n    <limit lower="-1.57" upper="1.57" effort="10.0" velocity="1.0"/>\n  </joint>\n\n  <link name="head">\n    <visual>\n      <geometry>\n        <sphere radius="0.1"/>\n      </geometry>\n      <material name="skin">\n        <color rgba="0.9 0.8 0.7 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Eyes (for expression) --\x3e\n  <joint name="eye_left_joint" type="fixed">\n    <parent link="head"/>\n    <child link="eye_left"/>\n    <origin xyz="0.03 0.05 0.0" rpy="0 0 0"/>\n  </joint>\n\n  <link name="eye_left">\n    <visual>\n      <geometry>\n        <sphere radius="0.015"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  <joint name="eye_right_joint" type="fixed">\n    <parent link="head"/>\n    <child link="eye_right"/>\n    <origin xyz="0.03 -0.05 0.0" rpy="0 0 0"/>\n  </joint>\n\n  <link name="eye_right">\n    <visual>\n      <geometry>\n        <sphere radius="0.015"/>\n      </geometry>\n      <material name="black">\n        <color rgba="0 0 0 1"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Mouth (for expression) --\x3e\n  <joint name="mouth_joint" type="revolute">\n    <parent link="head"/>\n    <child link="mouth"/>\n    <origin xyz="0.0 0.0 -0.05" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-0.2" upper="0.2" effort="1.0" velocity="0.5"/>\n  </joint>\n\n  <link name="mouth">\n    <visual>\n      <geometry>\n        <box size="0.03 0.01 0.01"/>\n      </geometry>\n      <material name="pink">\n        <color rgba="1.0 0.7 0.7 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- Arms --\x3e\n  <joint name="shoulder_left_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="upper_arm_left"/>\n    <origin xyz="0.0 0.15 0.15" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="10.0" velocity="1.0"/>\n  </joint>\n\n  <link name="upper_arm_left">\n    <visual>\n      <geometry>\n        <cylinder radius="0.03" length="0.15"/>\n      </geometry>\n      <material name="light_blue">\n        <color rgba="0.5 0.7 1.0 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  <joint name="shoulder_right_joint" type="revolute">\n    <parent link="torso"/>\n    <child link="upper_arm_right"/>\n    <origin xyz="0.0 -0.15 0.15" rpy="0 0 0"/>\n    <axis xyz="0 1 0"/>\n    <limit lower="-1.57" upper="1.57" effort="10.0" velocity="1.0"/>\n  </joint>\n\n  <link name="upper_arm_right">\n    <visual>\n      <geometry>\n        <cylinder radius="0.03" length="0.15"/>\n      </geometry>\n      <material name="light_blue">\n        <color rgba="0.5 0.7 1.0 1.0"/>\n      </material>\n    </visual>\n  </link>\n\n  \x3c!-- HRI Sensors --\x3e\n  <gazebo reference="head">\n    <sensor name="face_camera" type="camera">\n      <always_on>true</always_on>\n      <update_rate>30</update_rate>\n      <camera>\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n      <plugin name="face_camera_controller" filename="libgazebo_ros_camera.so">\n        <frame_name>head</frame_name>\n        <topic_name>face_camera/image_raw</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  <gazebo reference="torso">\n    <sensor name="microphone_array" type="ray">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>8</samples>\n            <resolution>1.0</resolution>\n            <min_angle>-3.14159</min_angle>\n            <max_angle>3.14159</max_angle>\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>5.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="microphone_controller" filename="libgazebo_ros_audio_device.so">\n        <frame_name>torso</frame_name>\n        <topic_name>audio_commands</topic_name>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Differential Drive Controller --\x3e\n  <gazebo>\n    <plugin name="diff_drive" filename="libgazebo_ros_diff_drive.so">\n      <left_joint>wheel_left_joint</left_joint>\n      <right_joint>wheel_right_joint</right_joint>\n      <wheel_separation>0.5</wheel_separation>\n      <wheel_diameter>0.2</wheel_diameter>\n      <command_topic>cmd_vel</command_topic>\n      <odometry_topic>odom</odometry_topic>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_link</robot_base_frame>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- Social Behavior Controller --\x3e\n  <gazebo reference="base_link">\n    <plugin name="social_controller" filename="libsocial_behavior_plugin.so">\n      <face_camera_topic>face_camera/image_raw</face_camera_topic>\n      <audio_topic>audio_commands</audio_topic>\n      <motor_topic>servo_commands</motor_topic>\n      <behavior_config>/config/social_behavior.yaml</behavior_config>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,o.jsx)(n.h2,{id:"hri-pipeline-diagram",children:"HRI Pipeline Diagram"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph TD\n    A[Human Input] --\x3e B[Voice Recognition]\n    A --\x3e C[Gesture Recognition]\n    A --\x3e D[Tactile Input]\n    B --\x3e E[Multimodal Fusion]\n    C --\x3e E\n    D --\x3e E\n    E --\x3e F[Intent Interpretation]\n    F --\x3e G[Social Behavior Selection]\n    G --\x3e H[Robot Response]\n    H --\x3e I[Human Feedback]\n    I --\x3e A\n\n    style A fill:#ff9999\n    style E fill:#99ff99\n    style H fill:#99ccff\n"})}),"\n",(0,o.jsx)(n.h2,{id:"hri-modalities-comparison",children:"HRI Modalities Comparison"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"Modality"}),(0,o.jsx)(n.th,{children:"Advantages"}),(0,o.jsx)(n.th,{children:"Disadvantages"}),(0,o.jsx)(n.th,{children:"Use Cases"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Voice"}),(0,o.jsx)(n.td,{children:"Natural, hands-free"}),(0,o.jsx)(n.td,{children:"Affected by noise, privacy concerns"}),(0,o.jsx)(n.td,{children:"Navigation, information queries"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Gesture"}),(0,o.jsx)(n.td,{children:"Intuitive, visual"}),(0,o.jsx)(n.td,{children:"Requires line of sight, cultural differences"}),(0,o.jsx)(n.td,{children:"Pointing, directional commands"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Touch"}),(0,o.jsx)(n.td,{children:"Direct, intimate"}),(0,o.jsx)(n.td,{children:"Hygiene concerns, limited expressiveness"}),(0,o.jsx)(n.td,{children:"Button presses, emergency stops"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Face Expression"}),(0,o.jsx)(n.td,{children:"Emotional connection"}),(0,o.jsx)(n.td,{children:"Requires display/actuators"}),(0,o.jsx)(n.td,{children:"Social interaction, feedback"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Proximity"}),(0,o.jsx)(n.td,{children:"Passive, safe"}),(0,o.jsx)(n.td,{children:"Limited control options"}),(0,o.jsx)(n.td,{children:"Welcome, attention getting"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"Eye Contact"}),(0,o.jsx)(n.td,{children:"Attention, engagement"}),(0,o.jsx)(n.td,{children:"Cultural sensitivity"}),(0,o.jsx)(n.td,{children:"Focus, confirmation"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"key-terms",children:"Key Terms"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"HRI"}),": Human-Robot Interaction - study of human-robot communication"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multimodal Interaction"}),": Using multiple input/output modalities simultaneously"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Robotics"}),": Robots designed for human interaction and companionship"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Affective Computing"}),": Computing that relates to emotions and feelings"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Proxemics"}),": Study of personal space and spatial relationships"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Embodied Cognition"}),": Idea that cognitive processes are influenced by body"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Theory of Mind"}),": Ability to attribute mental states to others"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Social Presence"}),": Feeling that robot is a social actor"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"learning-checkpoints",children:"Learning Checkpoints"}),"\n",(0,o.jsx)(n.h3,{id:"quiz-questions",children:"Quiz Questions"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"What are the three key principles of effective Human-Robot Interaction?"}),"\n",(0,o.jsx)(n.li,{children:"Name four different modalities for human-robot interaction."}),"\n",(0,o.jsx)(n.li,{children:"How does multimodal fusion improve HRI systems?"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"practical-exercise",children:"Practical Exercise"}),"\n",(0,o.jsx)(n.p,{children:"Implement a multimodal interaction system that combines voice commands and gesture recognition to control a robot's movement."}),"\n",(0,o.jsx)(n.h2,{id:"hands-on-exercise",children:"Hands-On Exercise"}),"\n",(0,o.jsx)(n.p,{children:"Create a ROS 2 node that integrates voice recognition, gesture recognition, and social behavior systems to create a natural human-robot interaction experience."}),"\n",(0,o.jsx)(n.h2,{id:"personalization",children:"Personalization"}),"\n",(0,o.jsxs)("div",{className:"personalization-options",children:[(0,o.jsx)("h3",{children:"Adjust Learning Path:"}),(0,o.jsx)("button",{onClick:()=>setDifficulty("beginner"),children:"Beginner"}),(0,o.jsx)("button",{onClick:()=>setDifficulty("intermediate"),children:"Intermediate"}),(0,o.jsx)("button",{onClick:()=>setDifficulty("advanced"),children:"Advanced"})]}),"\n",(0,o.jsx)(n.h2,{id:"translation",children:"Translation"}),"\n",(0,o.jsx)("div",{className:"translation-controls",children:(0,o.jsx)("button",{onClick:()=>translateToUrdu(),children:"\u0627\u0631\u062f\u0648 \u0645\u06cc\u06ba \u062a\u0631\u062c\u0645\u06c1 \u06a9\u0631\u06cc\u06ba"})})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}}}]);